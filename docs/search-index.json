[

{
  "title": "フーリエ変換の基礎：RoPEを理解するための数学的準備",
  "url": "/articles/fourier-transform/",
  "date": "2026.02.08",
  "description": "RoPE（Rotary Position Embedding）の理解に必要なフーリエ変換の数学的背景を、直感的な説明と厳密な数式の両面から解説します。",
  "content": "本記事では、Transformerの位置エンコーディング手法であるRoPE（Rotary Position Embedding）を深く理解するために必要な、フーリエ変換の数学的背景を解説します。直感的な理解と数学的な厳密さの両方を大切にしながら進めていきます。 フーリエ変換の基本アイデア フーリエ変換の核心は、次の一文に集約されます： 「どんな複雑な波も、単純な波（sin/cos）の重ね合わせで表現できる」 例えば、ピアノの「ド」の音は、基本周波数の波と、その整数倍の周波数を持つ倍音が重ね合わさって作られています。異なる楽器で同じ「ド」を弾いても音色が違うのは、含まれる周波数成分の比率（スペクトル）が異なるからです。 数学的定式化：フーリエ級数 周期 $2\\pi$ を持つ任意の関数 $f(x)$ は、以下のフーリエ級数で表現できます： $$ f(x) = \\frac{a_0}{2} + \\sum_{n=1}^{\\infty} \\left( a_n \\cos(nx) + b_n \\sin(nx) \\right) $$ ここで、フーリエ係数 $a_n$, $b_n$ は以下の積分で求められます： $$ a_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\cos(nx) \\, dx $$ $$ b_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} f(x) \\sin(nx) \\, dx $$ なぜこの形で表現できるのか：直交性 sin関数とcos関数がフーリエ級数の基底として選ばれる理由は、その直交性にあります。関数の内積を以下のように定義したとき： $$ \\langle f, g \\rangle = \\int_{-\\pi}^{\\pi} f(x) g(x) \\, dx $$ sin関数とcos関数は以下の直交関係を満たします： $$ \\int_{-\\pi}^{\\pi} \\cos(mx) \\cos(nx) \\, dx = \\begin{cases} \\pi & (m = n \\neq 0) \\\\ 0 & (m \\neq n) \\end{cases} $$ $$ \\int_{-\\pi}^{\\pi} \\sin(mx) \\sin(nx) \\, dx = \\begin{cases} \\pi & (m = n \\neq 0) \\\\ 0 & (m \\neq n) \\end{cases} $$ $$ \\int_{-\\pi}^{\\pi} \\cos(mx) \\sin(nx) \\, dx = 0 \\quad (\\forall m, n) $$ この直交性により、各周波数成分を独立に抽出でき、異なる周波数同士が干渉しません。これは、3次元空間でx軸・y軸・z軸が直交していることで、各座標成分を独立に扱えることと本質的に同じです。 オイラーの公式と複素指数関数表現 フーリエ変換をより簡潔に扱うために、オイラーの公式を導入します： $$ e^{i\\theta} = \\cos\\theta + i\\sin\\theta $$ この公式を使うと、sin と cos を統一的に扱えます： $$ \\cos\\theta = \\frac{e^{i\\theta} + e^{-i\\theta}}{2}, \\quad \\sin\\theta = \\frac{e^{i\\theta} - e^{-i\\theta}}{2i} $$ フーリエ級数は複素指数関数を使って以下のように書き換えられます： $$ f(x) = \\sum_{n=-\\infty}^{\\infty} c_n e^{inx} $$ ここで複素フーリエ係数は： $$ c_n = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} f(x) e^{-inx} \\, dx $$ 複素平面での解釈 $e^{i\\theta}$ は複素平面上の単位円周上の点を表し、$\\theta$ が増加すると反時計回りに回転します。これが後述するRoPEの「回転」と直接つながります。 周波数の役割：マルチスケール表現 フーリエ級数で複数の周波数を使う理由は、異なるスケールの情報を同時に捉えるためです。 周波数 捉える情報 例 低周波（nが小さい） 大域的な傾向 文章全体の流れ 中周波 中程度の構造 段落レベルの関係 高周波（nが大きい） 局所的な変動 隣接単語の関係 これらを組み合わせることで、あらゆるスケールの情報を同時に表現できます。 RoPEとフーリエ変換の数学的対応 RoPE（Rotary Position Embedding）は、位置情報をベクトルの回転として埋め込む手法です。フーリエ変換との対応を見ていきましょう。 RoPEの回転行列 RoPEでは、位置 $m$ において、ベクトルの各2次元ペア $(x_{2i}, x_{2i+1})$ に対して以下の回転行列を適用します： $$ R_m^{(i)} = \\begin{pmatrix} \\cos(m\\theta_i) & -\\sin(m\\theta_i) \\\\ \\sin(m\\theta_i) & \\cos(m\\theta_i) \\end{pmatrix} $$ ここで $\\theta_i$ は各ペアに固有の周波数パラメータで、以下のように定義されます： $$ \\theta_i = 10000^{-2i/d} $$ $d$ はモデルの次元数です。$i$ が大きくなると $\\theta_i$ は小さくなり、低周波の回転となります。 複素数表現との対応 2次元ベクトル $(x, y)$ を複素数 $z = x + iy$ と見なすと、回転行列の適用は複素数の乗算に対応します： $$ z' = z \\cdot e^{im\\theta_i} = (x + iy)(\\cos(m\\theta_i) + i\\sin(m\\theta_i)) $$ これはまさにオイラーの公式の形であり、RoPEがフーリエ変換と同じ数学的構造を持つことを示しています。 相対位置の内積表現 RoPEの重要な性質として、位置 $m$ のクエリと位置 $n$ のキーの内積が、相対位置 $(m-n)$ のみに依存するという点があります： $$ \\langle R_m q, R_n k \\rangle = \\langle R_{m-n} q, k \\rangle $$ これは回転行列の性質 $R_m^T R_n = R_{n-m}$ から導かれます。各ペアについて： $$ (R_m^{(i)})^T R_n^{(i)} = R_{n-m}^{(i)} $$ 内積に寄与する各ペアの成分は $\\cos((m-n)\\theta_i)$ の形となり、これはフーリエ級数における周波数成分と同じ構造です。 一意性の保証 複数の周波数を組み合わせることで、任意の位置差を一意に区別できます。これを具体例で見てみましょう。 位置差1と位置差7の区別 3つの周波数ペアで位置差を比較します： 位置差 $\\theta_0 = 1.0$ $\\theta_1 = 0.1$ $\\theta_2 = 0.01$ 1 $\\cos(1.0) \\approx 0.54$ $\\cos(0.1) \\approx 0.995$ $\\cos(0.01) \\approx 0.99995$ 7 $\\cos(7.0) \\approx 0.75$ $\\cos(0.7) \\approx 0.765$ $\\cos(0.07) \\approx 0.998$ 高周波ペア（$\\theta_0$）では値が似ていますが、中・低周波ペア（$\\theta_1$, $\\theta_2$）では明確に異なります。全てのペアを組み合わせることで、一意の識別が可能になります。 住所システムとの類似 この仕組みは住所システムと似ています： 国（低周波）：大きな範囲を区別 都道府県（中周波）：中程度の範囲を区別 番地（高周波）：最も細かい位置を区別 RoPEも同様に、低周波ペアは「だいたい1000トークン以内か否か」、高周波ペアは「隣接トークンか2つ離れているか」といった異なるスケールの位置関係を捉えます。 まとめ フーリエ変換とRoPEの数学的なつながりを整理すると： フーリエ変換 RoPE $e^{inx}$ による周波数分解 $e^{im\\theta_i}$ による位置エンコーディング 複数周波数の重ね合わせ 複数の $\\theta_i$ ペアの組み合わせ 直交基底による一意表現 回転角の組み合わせによる一意な位置表現 低周波：大域的構造 小さい $\\theta_i$：遠距離位置関係 高周波：局所的構造 大きい $\\theta_i$：近距離位置関係 RoPEは、フーリエ変換の「複数の周波数成分を組み合わせて任意の情報を表現する」という本質的なアイデアを、Transformerの位置エンコーディングに応用した手法と言えます。 参考文献 Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864. 3Blue1Brown. But what is the Fourier Transform? A visual introduction. YouTube."
},

{
  "title": "Qwen3-VL-Embedding 技術詳細",
  "url": "/articles/qwen3-vl-embedding/",
  "date": "2026.02.08",
  "description": "Qwen3-VL-Embeddingの特徴的技術（DeepStack、Interleaved-MRoPE、MRL、Multi-stage Training）を数式レベルで解説します。",
  "content": "本記事では、Qwen3-VL-Embeddingの特徴的技術を数式レベルで解説します。ViTの基礎知識を前提としています。 技術サマリー 技術 概要 効果 DeepStack ViTの複数層からDecoder各層へ特徴を融合 OCR・細部認識が大幅改善（DocVQA +6.7pt） Interleaved-MRoPE 3軸（時間・高さ・幅）の位置を交互配置でエンコード 1Mトークン検索精度 &gt;99.5% MRL 複数次元で同時に損失計算し、任意次元で切り取り可能に 64次元でも85%の性能を維持 Multi-stage Training 弱教師 → 強教師 → SLERPマージの3段階学習 汎化と特化のバランス調整 EOS Pooling Causal Attentionで末尾トークンに情報集約 可変長入力から固定長Embeddingを生成 モデル概要 項目 2B 8B ベースモデルQwen3-VL-2B-InstructQwen3-VL-8B-Instruct Transformer層28層36層 Hidden次元 $d$20484096 Attention Head1632 Head次元 $d_h$128128 コンテキスト長32K tokens32K tokens MRL対応次元64, 128, 256, 512, 1024, 204864〜4096 全体アーキテクチャ 入力 → 入力処理 → Interleaved-MRoPE → Transformer (DeepStack) → EOS抽出 → L2 Norm → Embedding 詳細: テキスト → Embedding Layer → (seq, seq, seq) 画像 → ViT + DeepStack → (0, row, col) 動画 → フレーム → ViT + DeepStack → (frame, row, col) ↓ Transformer + DeepStack Fusion ↓ EOS hidden state → L2 Norm → Embedding Vector DeepStack 解決する問題 従来のVLMは、ViTの最終層出力のみをDecoder（言語モデル部分）に渡します。 【従来のViT統合】 ViT Layer 1 → (低レベル特徴: エッジ、テクスチャ) ↓ ViT Layer 12 → (中レベル特徴: パーツ、パターン) ↓ ViT Layer 24 → (高レベル特徴: 物体、意味) ─→ Decoderへ ↑ここだけ使用 問題: - 低レベル特徴（文字の輪郭、線の太さ）が最終層で失われる - OCR、細かい空間関係の理解が困難 DeepStackの解決策 ViTの複数層から特徴を抽出し、LMM Decoderの異なる層に融合します。 【DeepStack】 ViT Encoder LMM Decoder ├─ 浅い層 ────────────────────→ Decoder 初期層へ融合 ├─ 中間層 ────────────────────→ Decoder 中間層へ融合 └─ 深い層 ────────────────────→ Decoder 後期層へ融合 数式表現 ViTの各層出力を $V^{(l)}$ とします（$l = 1, ..., L_{\\text{vit}}$）。 従来手法: $$ H^{(0)}_{\\text{visual}} = \\text{Proj}(V^{(L_{\\text{vit}})}) $$ 最終層のみをDecoderの入力層に渡します。 DeepStack: $$ H^{(k)}_{\\text{Dec}} = H^{(k)}_{\\text{Dec}} + \\text{Proj}_k(V^{(\\phi(k))}) $$ ここで: $\\phi(k)$: Decoder層 $k$ に対応するViT層のインデックス $\\text{Proj}_k$: 層ごとの線形投影 具体的なマッピング（推定） ViT層 捉える特徴 融合先Decoder層 1〜8エッジ、テクスチャ、色1〜10 9〜16パーツ、局所パターン11〜20 17〜24物体、シーン、意味21〜28 (or 36) 効果（アブレーション） 構成 TextVQA DocVQA ChartQA 最終層のみ56.731.745.2 DeepStack60.138.452.1 特にOCR依存タスク（DocVQA: +6.7pt）で顕著な改善が見られます。 Interleaved-MRoPE 解決する問題 標準RoPEは1次元の位置情報のみをエンコードします。 【標準RoPE】 位置 p ∈ ℤ → 回転角 θ = p × base_freq 問題: - 画像の2D空間構造を表現できない - 動画の時間+空間の3D構造を表現できない MRoPEの基本概念 Multi-dimensional RoPE（MRoPE）は、位置情報を3軸で表現します。 $$ \\text{pos} = (t, h, w) $$ $t$: 時間軸（テキストではシーケンス位置、動画ではフレーム番号） $h$: 高さ軸（行位置） $w$: 幅軸（列位置） モダリティ別の位置ID変換 モダリティ 元の次元 3軸への変換 例 テキスト1D position $p$$(p, p, p)$位置3 → $(3, 3, 3)$ 画像2D $(row, col)$$(0, row, col)$$(1, 2)$ → $(0, 1, 2)$ 動画3D $(frame, row, col)$$(frame, row, col)$フレーム5の$(1,2)$ → $(5, 1, 2)$ Interleaved配置 vs Block配置 Block配置（従来、不採用）: 次元インデックス i を連続的に分割: i ∈ [0, d_h/6) → t軸用 i ∈ [d_h/6, d_h/3) → h軸用 i ∈ [d_h/3, d_h/2) → w軸用 問題: 高周波成分がt軸に集中 - 回転角 θ_i = pos × base^(-2i/d_h) は i が小さいほど高周波 - t軸: 高周波のみ（近距離に敏感、遠距離に鈍感） - w軸: 低周波のみ（近距離に鈍感、遠距離に敏感） Interleaved配置（採用）: 次元インデックス i を交互に割り当て: i % 3 == 0 → t軸用 i % 3 == 1 → h軸用 i % 3 == 2 → w軸用 効果: 各軸に高周波〜低周波が均等配分 → 全軸で近距離・遠距離の両方を表現可能 数式表現 Head次元 $d_h = 128$、回転ペア数 $d_h/2 = 64$。 位置 $(t, h, w)$ に対する各ペア $i$ での回転角: $$ \\theta_i = \\begin{cases} t \\cdot \\text{base}^{-2\\lfloor i/3 \\rfloor \\cdot 3 / d_h} & \\text{if } i \\mod 3 = 0 \\text{ (t軸)} \\\\ h \\cdot \\text{base}^{-2\\lfloor i/3 \\rfloor \\cdot 3 / d_h} & \\text{if } i \\mod 3 = 1 \\text{ (h軸)} \\\\ w \\cdot \\text{base}^{-2\\lfloor i/3 \\rfloor \\cdot 3 / d_h} & \\text{if } i \\mod 3 = 2 \\text{ (w軸)} \\end{cases} $$ ここで $\\text{base} = 10000$（標準RoPEと同じ）。 効果 指標 Block配置 Interleaved配置 1Mトークン検索精度&lt;50%&gt;99.5% 動画ベンチマークベースライン+1〜2pt MRL（Matryoshka Representation Learning） 解決する問題 【従来】 出力次元: 4096次元（固定） 問題: - 軽量用途（リアルタイム検索）には過剰 - 次元削減すると性能が大幅劣化 - 用途ごとに別モデルが必要 MRLの解決策 1つのモデルで任意次元のEmbeddingを生成可能にします。 フル次元: [d₀, d₁, d₂, ..., d₂₀₄₇] 先頭N次元を切り取り: 64次元: [d₀, ..., d₆₃] 256次元: [d₀, ..., d₂₅₅] 1024次元: [d₀, ..., d₁₀₂₃] 2048次元: [d₀, ..., d₂₀₄₇] → 切り取っても意味のあるEmbeddingになる 数式表現 学習時、複数の次元で同時に損失計算します。 MRL損失関数: $$ \\mathcal{L}_{\\text{MRL}} = \\sum_{m \\in \\mathcal{M}} c_m \\cdot \\mathcal{L}_{\\text{InfoNCE}}(z_{1:m}) $$ ここで: $\\mathcal{M} = \\{64, 128, 256, 512, 1024, 2048\\}$: 学習する次元セット $c_m$: 次元 $m$ の重み $z_{1:m}$: Embeddingベクトルの先頭 $m$ 次元 InfoNCE損失（各次元で計算）: $$ \\mathcal{L}_{\\text{InfoNCE}}(z_{1:m}) = -\\log \\frac{\\exp(\\text{sim}(q_{1:m}, d^+_{1:m}) / \\tau)}{\\sum_{j} \\exp(\\text{sim}(q_{1:m}, d^j_{1:m}) / \\tau)} $$ なぜこれで機能するか 低次元で損失を計算するため、最も重要な情報が先頭に集約される 高次元では追加の詳細情報が格納される 結果として、次元数に応じた階層的な情報構造が形成 推論時の使用 # フル次元（最高精度） embedding_full = model.encode(text) # [2048] # 低次元（高速、省メモリ） embedding_64 = embedding_full[:64] # 先頭64次元を切り取り # 必ず再正規化 embedding_64 = embedding_64 / np.linalg.norm(embedding_64) 注意: 切り取り後は再度L2正規化が必要です。 次元別の性能（MMEB-V2, 8Bモデル） 次元 相対性能 ストレージ ユースケース 64~85%0.25KB超高速粗検索 256~92%1KBモバイル向け 1024~97%4KB標準検索 4096100%16KB最高精度 Multi-stage Training 3段階学習パイプライン Stage 1: Weakly Supervised Pretraining ↓ Stage 2: Supervised Fine-Tuning ↓ Stage 3: Model Merging (SLERP) Stage 1: Weakly Supervised Pretraining 目的: 大規模な弱教師データで基礎能力を獲得。 LLMで生成した合成ペアデータ（数百万〜数十億ペア） 多言語（100+言語） マルチタスク（検索、分類、クラスタリング、意味的類似性） Stage 2: Supervised Fine-Tuning 目的: 高品質データで識別能力を向上。 人手でラベル付けされた高品質ペア Hard Negativesを含む（類似しているが正解でないサンプル） 少量（Stage 1より大幅に少ない） Stage 3: Model Merging (SLERP) 目的: 汎化性能と特化性能のバランス調整。 Spherical Linear Interpolation (SLERP) で2つのモデルを補間します。 $$ \\theta_{\\text{merged}} = \\frac{\\sin((1-\\alpha)\\Omega)}{\\sin(\\Omega)} \\theta_1 + \\frac{\\sin(\\alpha \\Omega)}{\\sin(\\Omega)} \\theta_2 $$ ここで $\\Omega = \\arccos(\\theta_1 \\cdot \\theta_2 / (||\\theta_1|| \\cdot ||\\theta_2||))$。 model_general: Stage 1後のモデル（広範なタスクに対応） model_specialized: Stage 2後のモデル（特定タスクに最適化） α = 0.3 の場合: merged = slerp(0.3, model_general, model_specialized) → 汎化70% + 特化30%のバランス なぜこの順序か Stage 1 のみ: 広範だが浅い理解 → 「なんとなく似てる」は分かるが、細かい違いを識別できない Stage 2 のみ: 深いが狭い理解（過学習リスク） → 学習データに特化しすぎ、未見データに弱い Stage 1 → 2: 広範かつ深い理解 → 基礎を持った上で識別能力を追加 Stage 3: 最終調整 → 汎化と特化のバランスを用途に応じて調整 EOS Pooling 解決する問題 入力長に依存しない固定長表現が必要です。 【Mean Pooling】 全トークンの平均: (h₁ + h₂ + ... + hₙ) / n 問題: - パディングトークンの処理が必要 - 重要でないトークンも等重みで含まれる EOSトークンによる代表ベクトル Causal Attention（GPT系）では、各トークンはそれ以前のトークンのみを参照できます。 入力: [T₁, T₂, ..., Tₙ, &lt;EOS&gt;] Attention Pattern: T₁ → [T₁] T₂ → [T₁, T₂] ... Tₙ → [T₁, T₂, ..., Tₙ] EOS → [T₁, T₂, ..., Tₙ, EOS] ← 全トークンを参照可能 EOSの hidden state は、自然にシーケンス全体の情報を集約しています。 数式表現 最終層の hidden states $H \\in \\mathbb{R}^{n \\times d}$ から EOS 位置を抽出: $$ h_{\\text{eos}} = H[\\text{seq\\_len} - 1, :] $$ L2正規化: $$ e = \\frac{h_{\\text{eos}}}{||h_{\\text{eos}}||_2} $$ 動画処理 トークン数の計算 1フレーム: 解像度 (H, W) → パッチ数 = (H/14) × (W/14) 例: 224×224画像 → 16 × 16 = 256 パッチ 例: 448×448画像 → 32 × 32 = 1,024 パッチ 動画のトークン爆発 10フレーム × 1,024パッチ = 10,240 トークン 32フレーム × 1,024パッチ = 32,768 トークン → コンテキスト長超過 対策: 1. フレームサンプリング: fps=1.0（1秒1フレーム） 2. 最大フレーム制限: max_frames=64 3. 解像度の動的調整: 長い動画は低解像度で処理 時間位置のエンコード Interleaved-MRoPEの $t$ 軸がフレーム番号を表現: フレーム0: 全パッチの位置 = (0, row, col) フレーム1: 全パッチの位置 = (1, row, col) ... フレームN: 全パッチの位置 = (N, row, col) まとめ 技術 解決する問題 解決方法 DeepStackViT最終層のみでは細部が失われる複数層の特徴をDecoder各層に融合 Interleaved-MRoPE1D RoPEでは空間/時間を表現できない3軸を交互配置で均等に周波数配分 MRL Loss固定次元では用途に応じた調整不可複数次元で同時に損失計算 Multi-stage Training汎化と特化のトレードオフ3段階学習 + SLERPマージ EOS Pooling可変長入力から固定長出力が必要Causal Attentionで末尾に情報集約 参考文献 Alibaba Qwen Team. (2025). Qwen3-VL-Embedding Technical Report. arXiv:2601.04720. Wang, P., et al. (2024). Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution. arXiv:2409.12191. Kusupati, A., et al. (2022). Matryoshka Representation Learning. NeurIPS 2022. Su, J., et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864."
},

{
  "title": "位置エンコーディング解説：RoPE から Interleaved-MRoPE まで",
  "url": "/articles/rope-positional-encoding/",
  "date": "2026.02.08",
  "description": "RoPE（Rotary Positional Embedding）からMRoPE、Interleaved-MRoPEまで、Transformerの位置エンコーディング技術を図解と具体例で解説します。",
  "content": "Transformerは入力の順序を認識できません（Self-Attentionは順序に依存しない）。そのため、位置情報を明示的に与える必要があります。本記事では、RoPE（Rotary Positional Embedding）を中心に、マルチモーダル対応のMRoPE、Interleaved-MRoPEまでを解説します。 なぜ位置エンコーディングが必要か 「今日 は 晴れ」と「晴れ は 今日」は、位置情報がないとTransformerには同じに見えてしまいます。単語の順序が意味を決定する言語において、位置情報は不可欠です。 RoPE (Rotary Positional Embedding) 基本アイデア RoPEは、埋め込みベクトルの2次元ペアを「2D平面上の点」と見なし、位置に応じて回転させる手法です。 ステップ1: 埋め込みを2次元ペアに分ける 埋め込みベクトル（例: 6次元） [x₀, x₁, x₂, x₃, x₄, x₅] ↓ ペアに分割 (x₀, x₁), (x₂, x₃), (x₄, x₅) ↓ 各ペアを2D平面上の点と見なす ステップ2: 位置に応じた角度で回転 位置 $p$ のトークンに対し、角度 $p \\times \\theta$ で回転を適用します： $$ \\begin{pmatrix} x' \\\\ y' \\end{pmatrix} = \\begin{pmatrix} \\cos(p\\theta) & -\\sin(p\\theta) \\\\ \\sin(p\\theta) & \\cos(p\\theta) \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} $$ なぜ回転が有効なのか：内積の性質 位置 $p$ のトークンと位置 $q$ のトークンの内積を計算すると、回転角度の差 $(p-q) \\times \\theta$ に依存します。つまり、絶対位置ではなく「相対位置」$(p-q)$ が自然にエンコードされるのです。 「今日」と「晴れ」の関係: 位置差 = 3 - 1 = 2 → 回転角度の差 = 2×θ 「は」と「晴れ」の関係: 位置差 = 3 - 2 = 1 → 回転角度の差 = 1×θ 距離が近いほど回転角度の差が小さい → 内積（類似度）が高くなる RoPE詳細解説 2次元ペアは1トークン内の話 2次元ペアへの分割は1つのトークンの埋め込みベクトル内で行われます。 1つのトークン「今日」の埋め込みベクトル（例: 768次元） [x₀, x₁, x₂, x₃, x₄, x₅, ..., x₇₆₆, x₇₆₇] ↓ 2次元ペアに分割（384ペア） (x₀, x₁) → ペア0: θ₀で回転 (x₂, x₃) → ペア1: θ₁で回転 (x₄, x₅) → ペア2: θ₂で回転 ... (x₇₆₆, x₇₆₇) → ペア383: θ₃₈₃で回転 各ペアは異なる周波数を持つ 周波数パラメータ $\\theta_i$ は以下のように定義されます： $$ \\theta_i = 10000^{-2i/d} $$ ペア番号 $\\theta_i$ の値 特性 $i=0$ $\\theta_0 = 1$ 高周波 → 近い位置の違いに敏感 $i=100$ $\\theta_{100} \\approx 0.05$ 中周波 $i=383$ $\\theta_{383} \\approx 0.0001$ 低周波 → 遠い位置の違いに敏感 なぜ1トークンの回転が他トークンとの位置関係につながるのか 回転は各トークンに対して行われますが、効果が現れるのはAttentionの内積計算時です。 ステップ1: 各トークンを自分の位置で回転（個別の操作） 「今日」(位置1) → 1×θ で回転 → q₁ 「は」 (位置2) → 2×θ で回転 → q₂ 「晴れ」(位置3) → 3×θ で回転 → q₃ ステップ2: Attentionで内積を計算（ここで位置関係が現れる） q₁ · q₃ の内積を計算すると... → 回転角度の差 (3-1)×θ = 2θ が内積に反映される → 「今日」と「晴れ」は2トークン離れている、という情報が含まれる 高周波・低周波と位置感度の関係 内積に $\\cos((p-q) \\times \\theta)$ が現れるため、周波数 $\\theta$ が位置差への感度を決めます。 高周波ペア（$\\theta=1.0$）の場合： 位置差$(p-q)$ 角度差 cos値 11.00.54 22.0-0.42 33.0-0.99 → 位置差1と2で内積が大きく変わる → 近距離に敏感 低周波ペア（$\\theta=0.01$）の場合： 位置差$(p-q)$ 角度差 cos値 10.010.99995 20.020.99980 1001.000.54 → 位置差1〜4では内積がほぼ変わらない → 長距離に敏感 複数周波数の組み合わせ もし高周波だけだと： 位置1と位置101で同じ回転角度になる可能性（周期の繰り返し） → 遠い位置を区別できない もし低周波だけだと： 位置1と位置2がほぼ同じ回転角度 → 近い位置を区別できない 両方組み合わせると： 高周波ペア → 近距離の位置関係を捉える 低周波ペア → 長距離の位置関係を捉える → 全てのスケールの位置関係を同時に表現できる これはフーリエ変換の考え方と同じです。複数の周波数の波を重ね合わせることで、任意の位置情報を表現できます。 なぜ「回転」が選ばれたのか：ノルム保存 回転が選ばれた理由はノルム（ベクトルの長さ）を保存するからです。 回転前のベクトル: (3, 4) → ノルム = √(3² + 4²) = 5 ↓ 45度回転 回転後のベクトル: (-0.7, 4.9) → ノルム = √(0.7² + 4.9²) = 5 → 回転してもベクトルの長さは変わらない 内積の公式は $\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| \\times |\\mathbf{b}| \\times \\cos(\\theta)$ なので、ノルムが変わると内積値が歪みます。回転なら意味情報（ノルム）を保ちながら位置情報（角度）を追加できます。 操作 ノルム 結果 加算で位置を追加変わる可能性意味情報が歪む スケーリング変わる意味情報が歪む 回転保存意味情報が保たれる MRoPE (Multi-dimensional RoPE) テキストは1次元（シーケンス位置のみ）ですが、画像・動画は複数次元の位置を持ちます。 モダリティ 位置情報 次元数 テキスト$p=1, 2, 3, ...$1次元 画像$(h, w)$2次元 動画$(t, h, w)$3次元 MRoPEの仕組み 動画フレームの位置: (t=5, h=10, w=20) 各次元ペアに対して、対応する軸の位置で回転: ペア0: 時間軸 → 5×θ₀ で回転 ペア1: 高さ軸 → 10×θ₁ で回転 ペア2: 幅軸 → 20×θ₂ で回転 ... 元のMRoPEの問題点 周波数を連続的に分割していたため、各軸が使える周波数帯が偏る（スペクトル不均衡）という問題がありました。 従来のMRoPE: 次元 0-20 → 時間軸用（低周波数帯） 次元 21-40 → 高さ軸用（中周波数帯） 次元 41-60 → 幅軸用（高周波数帯） → 長距離の関係を捉える能力が軸によって異なる Interleaved-MRoPE 解決策: 交互配置 周波数をラウンドロビンで交互に割り当てることで、各軸が均等に全周波数帯を使えるようにします。 従来のMRoPE: 次元: [0,1,2,...,20] [21,22,...,40] [41,42,...,60] 軸: [時間時間時間...] [高さ高さ高さ...] [幅幅幅...] Interleaved-MRoPE: 次元: [0] [1] [2] [3] [4] [5] [6] [7] [8] ... 軸: [時] [高] [幅] [時] [高] [幅] [時] [高] [幅] ... 具体例: 動画フレームの位置 (t=10, h=64, w=128) 次元0 → 時間軸 → 10 × θ₀ で回転 次元1 → 高さ軸 → 64 × θ₁ で回転 次元2 → 幅軸 → 128 × θ₂ で回転 次元3 → 時間軸 → 10 × θ₃ で回転 次元4 → 高さ軸 → 64 × θ₄ で回転 次元5 → 幅軸 → 128 × θ₅ で回転 ... (繰り返し) 効果 指標 従来のMRoPE Interleaved-MRoPE 256Kコンテキストでの動画理解性能低下安定 1Mトークンでのneedle検索&lt;50%&gt;99.5% 動画ベンチマークベースライン+1〜2ポイント RoPEとコサイン類似度の関係 RoPE系の位置エンコーディングは、Embeddingモデルでコサイン類似度が使える理由に繋がっています。 回転操作の特性 ノルム（ベクトルの長さ）を保存する 内積の計算で相対位置が自然に反映される なぜコサイン類似度と相性が良いのか $$ \\text{コサイン類似度} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| \\times |\\mathbf{b}|} = \\text{正規化された内積} $$ RoPEの回転はノルムを変えないため、内積ベースの類似度計算と自然に適合します。 まとめ RoPE（テキスト用） ↓ 位置次元を拡張 MRoPE（画像・動画用） ↓ 周波数配置を改善 Interleaved-MRoPE（Qwen3-VL採用） 手法 位置次元 用途 特徴 RoPE1次元テキスト相対位置を内積で表現 MRoPE2〜3次元画像・動画複数軸の位置を扱う Interleaved-MRoPE2〜3次元画像・動画周波数を均等配分、長文脈に強い 参考文献 Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864. Qwen Team. Qwen3-VL Technical Report. arXiv:2511.21631."
},

{
  "title": "Self-Attention 詳細ガイド：Q, K, V から Multi-Head まで",
  "url": "/articles/self-attention/",
  "date": "2026.02.08",
  "description": "TransformerのSelf-Attention機構を詳しく解説。Q, K, Vの役割、Multi-Head Attention、FFN、InfoNCE Lossまでを図解と具体例で説明します。",
  "content": "本記事では、TransformerのSelf-Attention機構を詳しく解説します。「なぜその仕組みが必要なのか」という動機から、具体的な計算の流れまでを丁寧に説明していきます。 Self-Attentionの目的 各トークンは独立したベクトルとして入力されるため、文脈（周囲のトークン）の情報がありません。「bank」という単語が銀行なのか土手なのかは、周囲の単語を見なければわかりません。 問題 Self-Attention による解決 トークンが孤立他のトークンの情報を取り込む 文脈がわからない文脈依存の意味を表現 多義語の曖昧さ周囲から正しい意味を判断 全体の流れ 入力: X（トークン列、各トークン d次元） [x₁, x₂, x₃, ...] ← 位置エンコーディング済み Step 1: Q, K, V を生成（線形変換） Q = X × Wq (Query: 「何を探しているか」) K = X × Wk (Key: 「自分は何を持っているか」) V = X × Wv (Value: 「実際の情報」) Step 2: RoPE回転を適用（位置情報の付与） Q' = RoPE(Q) K' = RoPE(K) V はそのまま（回転しない） Step 3: Attention Score を計算 Score = Q' × K'ᵀ / √d Step 4: Softmax で正規化 Attention Weight = softmax(Score) Step 5: Value の重み付け和 Output = Attention Weight × V 出力: 文脈を考慮した新しいベクトル列 Q, K, V の詳細 なぜ Q, K, V に分けるのか 単純にトークン同士の内積を取るだけでは、自分自身との類似度が常に最大になり、柔軟な注目パターンを学習できません。役割を分離することで柔軟な情報交換が可能になります。 Query: 「私は何を探している？」← 探す視点 Key: 「私は何を提供できる？」← 提供する視点 Value: 「私の実際の情報」← 渡す情報 同じ入力から異なる重み行列で分ける 同じ入力ベクトル X から: Q = X × Wq ← 重み行列 Wq K = X × Wk ← 重み行列 Wk（Wq とは別の値） V = X × Wv ← 重み行列 Wv（Wq, Wk とは別の値） Wq, Wk, Wv はすべて学習パラメータ（異なる値） 学習で獲得されること Wq が学ぶこと: 「何を探しているか」への変換 \"bank\" → 「金融 or 地理に関連するものを探している」 Wk が学ぶこと: 「何を提供できるか」への変換 \"money\" → 「金融に関連する情報を提供できる」 \"river\" → 「地理に関連する情報を提供できる」 マッチング: Q_bank · K_money = 高い（金融 ↔ 金融） Q_bank · K_river = 高い（地理 ↔ 地理） → 文脈で「どちらに注目するか」が決まる Multi-Head Attention なぜ Multi-Head が必要か 1つのヘッド（Single Head）では1種類の関係しか見れませんが、言語・画像には多様な関係があります。複数のヘッドで同時に観察することで、構文、意味、位置など多角的な情報収集が可能になります。 \"彼女は銀行に行った\" 必要な関係性: - 構文: 「彼女」→「行った」（主語-述語） - 参照: 「彼女」→ 文脈の人物 - 意味: 「銀行」→ 金融機関 or 土手？ - 位置: 隣接する単語の関係 1つのヘッドで全部は無理 → 128次元でも「1種類の関係」なら十分 → 16種類あれば多くの関係をカバー 次元の分割 入力: 1トークン = 2048次元ベクトル Q = X × Wq で Query を生成（2048次元） ↓ 16個のヘッドに分割（2048 ÷ 16 = 128次元/ヘッド） [d₀, d₁, ..., d₁₂₇, d₁₂₈, ..., d₂₅₅, ..., d₁₉₂₀, ..., d₂₀₄₇] └─── Head 1 ───┘ └─── Head 2 ───┘ └─── Head 16 ──┘ (128次元) (128次元) (128次元) 分身チームの比喩 入力トークン「今日」(2048次元) │ ↓ 16分割 ┌────────┼────────┬────────┬─────┬────────┐ ↓ ↓ ↓ ↓ ↓ 分身1 分身2 分身3 ... 分身16 (128dim) (128dim) (128dim) (128dim) │ │ │ │ ↓ ↓ ↓ ↓ 「構文的に 「意味的に 「位置的に 「別の 関係ある 関係ある 近いトークン 視点で トークンは？」トークンは？」は？」 は？」 │ │ │ │ └────────┴────────┴──────────────┘ │ ↓ 連結（合体） 「今日」の新しいベクトル (2048次元) = 16人の分身が集めた情報の統合 なぜ128次元でも十分か 128次元 = $2^{128}$ 通りの方向を区別可能 = $10^{38}$ 通り（宇宙の原子数より多い）。「構文関係」だけを表現するには十分すぎるため、次元数の問題より「何を見るか」の多様性が重要です。 Attention Score の計算 なぜスケーリング（$\\sqrt{d_k}$ で割る）が必要か 次元数が増えると内積の絶対値が大きくなります。大きい値がSoftmaxに入ると「勝者総取り」になり、1位以外の勾配がほぼ0になって学習が進みません。 $$ \\text{Score} = \\frac{Q \\times K^T}{\\sqrt{d_k}} $$ 統計的な理由 Q, K の各要素が平均0、分散1の分布だと仮定すると、$d_k$ 個の項の和の分散は $d_k$ に比例します。標準偏差 $\\sqrt{d_k}$ で割ると分散が1に正規化されます。 Softmax と重み付け なぜ Softmax を使うのか 全て正の値になる 合計が1.0になる（確率分布として扱える） 差を強調しつつ、全員に少しは重みを残す Softmax の計算例 Score = [2.1, 0.5, -0.3] ← 3トークンへの注目度（正規化前） Step 1: exp を取る exp([2.1, 0.5, -0.3]) = [8.17, 1.65, 0.74] Step 2: 合計で割る 合計 = 8.17 + 1.65 + 0.74 = 10.56 Weight = [8.17/10.56, 1.65/10.56, 0.74/10.56] = [0.77, 0.16, 0.07] → 合計 = 1.0（確率分布） → 大きいスコアほど大きい重み Value の重み付け和 Weight = [0.77, 0.16, 0.07] ← トークン1の注目度 V₁ = [0.2, 0.8, ...] ← トークン1のValue V₂ = [0.5, 0.3, ...] ← トークン2のValue V₃ = [0.1, 0.6, ...] ← トークン3のValue 出力₁ = 0.77×V₁ + 0.16×V₂ + 0.07×V₃ → 重みが大きいトークンの情報が多く混入 FFN（Feed Forward Network） なぜ FFN が必要か：線形と非線形 Self-Attentionの最終出力は、Valueベクトルの重み付き平均です。これは足し算と掛け算だけの「線形結合」であり、活性化関数を使いません。 あるトークンの出力 = 0.7×V₁ + 0.2×V₂ + 0.1×V₃ ↑ 線形結合（足し算と掛け算のみ） 線形変換だけでは、何層重ねても1層と同じです： 層1: y = W₁x 層2: z = W₂y = W₂(W₁x) = (W₂W₁)x = Wx → 結局1つの行列Wと同じ → 層を深くする意味がない FFNは活性化関数（GELU）で非線形性を加えることで、複雑なパターンを学習可能にします。 FFN の式と各記号の意味 $$ \\text{FFN}(x) = \\text{GELU}(x W_1 + b_1) W_2 + b_2 $$ 記号 意味 サイズ例 $x$入力ベクトル（Attentionの出力）2048次元 $W_1$第1層の重み行列2048 × 8192 $b_1$第1層のバイアス8192次元 $W_2$第2層の重み行列8192 × 2048 $b_2$第2層のバイアス2048次元 GELU活性化関数- 入力 x (2048次元) ↓ × W₁ + b₁ 隠れ層 (8192次元) ← 4倍に拡張 ↓ GELU 活性化後 (8192次元) ↓ × W₂ + b₂ 出力 (2048次元) ← 元に戻す GELU（活性化関数）の役割 GELU（Gaussian Error Linear Unit）は「柔らかいReLU」です： $$ \\text{GELU}(x) = x \\times \\Phi(x) $$ $\\Phi(x)$ は正規分布の累積分布関数（「xが正である確率」）です。 x が大きい正 → ほぼ x をそのまま出力 x が大きい負 → ほぼ 0 を出力 x が 0 付近 → 滑らかに遷移 x ReLU(x) GELU(x) -20-0.05 -10-0.16 000 110.84 221.95 なぜ ReLU ではなく GELU か 滑らかさ：ReLUは x=0 で角があるが、GELUは全域で滑らか → 勾配が安定 負の値も少し通す：ReLUは x&lt;0 で勾配0（学習が止まる）、GELUは小さい勾配が残る 確率的解釈：「この入力は重要か？」を確率的に判断 重みとバイアスはどう決まるか $W_1$, $W_2$, $b_1$, $b_2$ は学習で獲得されるパラメータです。 【初期化】ランダムな小さい値 W₁, W₂ = 平均0、標準偏差0.02程度のランダム値 b₁, b₂ = 0 または小さい値 【学習中】勾配降下法で更新 1. 入力を与えて出力を計算（順伝播） 2. 正解との誤差（Loss）を計算 3. 誤差を逆方向に伝播（逆伝播） 4. 各重みの勾配（∂Loss/∂W）を計算 5. 重みを更新：W ← W - 学習率 × 勾配 これを何億回も繰り返す バイアスの役割 y = Wx + b ↑ バイアス = 「基準点の調整」 b が正 → GELUの閾値が下がり、より通しやすくなる b が負 → GELUの閾値が上がり、より抑制しやすくなる FFN = 知識の保存場所 研究（Geva et al., 2021）によると、FFNは連想記憶として機能します： W₁の各行 = Key（パターン検出器） W₂の各列 = Value（対応する出力） 例： W₁のある行が「首都に関する文脈」を検出 ↓ GELU で活性化 W₂の対応する列が「首都の知識」を出力に加える FFN vs Attention の役割分担 Self-Attention FFN 何をするトークン間の情報を混ぜる各トークンを個別に変換 学ぶもの「誰を見るか」のパターン「何を出力するか」の知識 例「bank」は「money」を見る「Tokyo」→「日本の首都」 線形性重み付き平均（線形）GELU（非線形） RoPE との関係 Self-Attentionは全トークンを一度に比較するため、順番の情報がありません。「犬が猫を追いかけた」と「猫が犬を追いかけた」は同じトークン集合のため、区別できません。 RoPE の適用タイミング 1. Q, K, V を生成 Q = X × Wq K = X × Wk V = X × Wv 2. Q と K に RoPE を適用 ← ここで位置情報が入る Q' = RoPE(Q, position) K' = RoPE(K, position) 3. Attention Score を計算 ← ここで位置情報が効果を発揮 Score = Q' × K'ᵀ / √d_k （この内積計算で相対位置 (m-n) がスコアに反映される） 4. Softmax → 重み付け和 Output = softmax(Score) × V ← V は回転しない なぜ Q と K だけに適用するのか Q, K の役割: 「誰に注目するか」を決める → 位置によって注目パターンを変えたい V の役割: 「実際の情報」を渡す → 情報そのものは位置で変わらない InfoNCE Loss（対照学習） なぜ InfoNCE を使うのか Embeddingは「類似度」を学習したいですが、クラス数が膨大で相対的な順序が重要です。InfoNCEは正解との類似度を高く、不正解との類似度を低くする相対的な差を学習します。 InfoNCE の数式 $$ L = -\\log \\frac{\\exp(\\text{sim}(q, d^+) / \\tau)}{\\exp(\\text{sim}(q, d^+) / \\tau) + \\sum_i \\exp(\\text{sim}(q, d_i^-) / \\tau)} $$ 要素 役割 $\\text{sim}(q, d^+)$正解との類似度を高くしたい $\\text{sim}(q, d^-)$不正解との類似度を低くしたい $\\exp(\\cdot/\\tau)$差を強調（$\\tau$小→シャープ） $-\\log$確率→損失に変換 温度パラメータ $\\tau$ の役割 【τ が大きい（高温）】 softmax: [0.32, 0.30, 0.22, 0.16] ← なだらか - 分布が均等に近い - 多くのネガティブから学べる - 学習が安定 【τ が小さい（低温）】 softmax: [0.70, 0.26, 0.03, 0.00] ← シャープ - 正解が際立つ - Hard Negative に集中 - 識別力が高い まとめ Self-Attention の核心: 1. Q, K, V への変換 → 「探す」「提供」「情報」を分離 2. 内積で類似度計算 → どのトークンに注目するか決定 3. スケーリング（√d_k） → 勾配を安定させる 4. Softmax で正規化 → 確率分布として扱える 5. Value の重み付け和 → 関連トークンの情報を取り込む 6. Multi-Head で並列化 → 多角的な注目パターン（分身チーム） 7. RoPE で位置情報 → 相対位置が内積に反映 8. FFN で知識処理 → 非線形変換と知識の保存 参考文献 Vaswani, A., et al. (2017). Attention Is All You Need. NeurIPS 2017. Geva, M., et al. (2021). Transformer Feed-Forward Layers Are Key-Value Memories. EMNLP 2021."
},

{
  "title": "ViT（Vision Transformer）画像埋め込み解説",
  "url": "/articles/vit-embedding/",
  "date": "2026.02.08",
  "description": "Vision Transformer（ViT）における画像の埋め込み方法を解説。テキストとの対比を通じて、パッチ分割・線形投影・位置エンコーディングの仕組みを理解します。",
  "content": "本記事では、Vision Transformer（ViT）がどのように画像をベクトル化するかを解説します。テキストの埋め込みと対比しながら、画像特有の処理を理解していきます。 テキストと画像の対比 テキスト: \"今日は晴れ\" → トークン化 → [\"今日\", \"は\", \"晴れ\"] → 埋め込み層 → ベクトル列 画像: [画像データ] → パッチ分割 → [パッチ1, パッチ2, ...] → 線形投影 → ベクトル列 画像では「パッチ」がトークンに相当します。 画像のパッチ分割 ステップ1: 画像をグリッド状に分割 元の画像 (224×224ピクセル) ┌────┬────┬────┬────┬────┬────┬────┐ │ │ │ │ │ │ │ │ ... ├────┼────┼────┼────┼────┼────┼────┤ │ │ │ │ │ │ │ │ ... ├────┼────┼────┼────┼────┼────┼────┤ ... 16×16ピクセルのパッチに分割 → 224 ÷ 16 = 14 → 14 × 14 = 196パッチ ステップ2: 各パッチをフラット化 1つのパッチ (16×16ピクセル、RGB 3チャンネル) → 16 × 16 × 3 = 768個の数値 パッチのピクセル値: ┌─────────────────┐ │ R G B R G B ... │ ← 1行目のピクセル │ R G B R G B ... │ ← 2行目のピクセル │ ... │ └─────────────────┘ ↓ フラット化 [r₀, g₀, b₀, r₁, g₁, b₁, ..., r₂₅₅, g₂₅₅, b₂₅₅] ← 768次元ベクトル 線形投影（Patch Embedding） テキストとの違い テキスト: トークンID → 埋め込み層（テーブル参照）→ ベクトル ※ 離散的なID → ルックアップ 画像: パッチ（768次元）→ 線形投影（行列積）→ ベクトル（768次元など） ※ 連続的な数値 → 行列演算 線形投影の仕組み $$ \\text{出力} = \\text{パッチベクトル} \\times W + b $$ パッチベクトル (768次元) 投影行列 W (768×768) 出力 (768次元) [r₀, g₀, b₀, ..., b₂₅₅] × [学習されるパラメータ] = [x₀, x₁, ..., x₇₆₇] 入力: 生のピクセル値（意味を持たない数値の羅列） 出力: 意味的な特徴を捉えたベクトル なぜテーブル参照ではなく行列積か テキスト 画像 入力の性質離散的（\"猫\" or \"犬\"、中間はない）連続的（0〜255の任意の値） パターン数有限（語彙5万語など）無限（パッチのパターンは無限） 埋め込み方法テーブルで全パターンを持てる関数（行列積）で変換する必要がある 全体の流れ 画像 (224×224×3) ↓ パッチ分割 (16×16) 196個のパッチ (各16×16×3 = 768ピクセル) ↓ フラット化 196個のベクトル (各768次元) ↓ 線形投影 (学習パラメータ) 196個の埋め込みベクトル (各768次元) ↓ [CLS]トークンを先頭に追加 197個のベクトル ↓ 位置エンコーディング追加 197個の位置情報付きベクトル ↓ Transformer Encoder 197個の出力ベクトル ↓ [CLS]トークンの出力を使用 1つの画像埋め込み (768次元) [CLS]トークンとは 画像全体を代表するための特別なトークンです。 入力: [CLS] [パッチ1] [パッチ2] ... [パッチ196] ↓ Transformer (Self-Attention) [CLS'] [パッチ1'] [パッチ2'] ... [パッチ196'] [CLS]トークンはAttentionで全パッチの情報を集約 → [CLS']が画像全体の表現になる なぜ[CLS]を使うのか 方法 説明 欠点 全パッチの平均 単純な平均プーリング 重要な部分も重要でない部分も同じ重み [CLS]トークン Attentionで重要なパッチに注目 学習で「どこを見るべきか」を獲得 位置エンコーディング（画像版） なぜ必要か パッチをフラット化すると位置情報が失われる: 元の画像: [顔] [空] [体] [木] フラット化後: [顔] [空] [体] [木] ← どれが隣同士かわからない 画像での位置エンコーディング 方法 説明 アプローチ 1次元の位置 パッチ0, パッチ1, パッチ2, ... テキストと同じRoPE的アプローチ 2次元の位置 パッチ(0,0), パッチ(0,1), パッチ(1,0), ... MRoPE的アプローチ（高さと幅を別々に） テキストと画像の埋め込み比較 項目 テキスト 画像 (ViT) 入力単位トークン（単語/サブワード）パッチ（16×16ピクセル） 入力形式離散的（ID）連続的（ピクセル値） 埋め込み方法テーブル参照線形投影（行列積） 位置情報1次元（シーケンス位置）1次元 or 2次元（行・列） 代表ベクトル[EOS]や平均[CLS]や平均 動的解像度（Qwen2-VL以降） 従来のViTは全ての画像を固定サイズにリサイズしていましたが、最新のモデルでは動的解像度に対応しています。 従来のViT: 全ての画像を224×224にリサイズ → 196パッチ固定 Qwen2-VL以降: 画像サイズに応じてパッチ数が変動 - 小さい画像: 少ないパッチ - 大きい画像: 多いパッチ（最大16,384パッチ） 解像度は28の倍数に調整 パッチサイズは14×14ピクセル 例 画像サイズ 448×448: → 448 ÷ 14 = 32 → 32 × 32 = 1,024パッチ 画像サイズ 224×224: → 224 ÷ 14 = 16 → 16 × 16 = 256パッチ マルチモーダル（テキスト+画像）の統合 テキスト: \"この画像に写っているのは\" 画像: [猫の画像] 処理: テキスト → トークン埋め込み → [T₁] [T₂] [T₃] [T₄] [T₅] 画像 → パッチ埋め込み → [I₁] [I₂] [I₃] ... [I₁₉₆] 統合（連結）: [T₁] [T₂] [T₃] [T₄] [T₅] [I₁] [I₂] [I₃] ... [I₁₉₆] ↑ テキストと画像を同じシーケンスに → Transformerで一緒に処理 → テキストと画像の関係を学習 まとめ テキスト埋め込み: 単語 → ID → テーブル参照 → ベクトル 画像埋め込み (ViT): 画像 → パッチ分割 → 線形投影 → ベクトル 共通点: - 入力を「トークン列」として扱う - Transformerで処理できる形式に変換 - 位置エンコーディングで位置情報を追加 違い: - テキストは離散（テーブル）、画像は連続（行列積） - 画像は2次元の位置情報を持つ 参考文献 Dosovitskiy, A., et al. (2020). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ICLR 2021. Wang, P., et al. (2024). Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution. arXiv:2409.12191."
},

{
  "title": "SuperRAG: 文書レイアウトを理解する次世代 RAG の仕組みと性能を徹底解説する",
  "url": "/articles/superRAG/",
  "date": "2025.12.10",
  "description": "文書のレイアウト構造ごと理解する「SuperRAG」の仕組みと性能を、図表付きで解説します。",
  "content": "SuperRAG: 文書レイアウトを理解する次世代 RAG の仕組みと性能を徹底解説する RAG（Retrieval-Augmented Generation）は、技術文書やマニュアル、研究資料の検索・QA に広く使われている。しかし従来の RAG は、文書を 「テキストチャンクの集合」 として扱うことが前提で、図や表、段組み、キャプションなどを一切考慮しない。そのため本文と図表の関係性を理解できず、重要な情報が落ちる。 論文 “SuperRAG: Beyond RAG with Layout-Aware Graph Modeling” は、文書をレイアウト構造ごと理解する RAG を提案する。本記事では、仕組みからベンチマーク結果までを図表付きで整理する。 従来 RAG の問題点 ― なぜ「構造」が必要なのか 従来の流れ： 文書を一定長でチャンク分割 チャンク単位でベクトル化 類似度の高いチャンクを検索 LLM に渡して回答生成 この方式の弱点： 図とキャプションが分離される 表（テーブル）と説明段落が離れる 段組み・上下関係などのレイアウト構造が消える 「図2の情報を基に表3の値と比較する」ような統合理解ができない 技術文書・論文・マニュアルは複雑な構造があるからこそ情報が正確に伝わる。そこを壊す従来 RAG では限界がある。 SuperRAG の中核アイデア：文書を “グラフ” として扱う SuperRAG は、文書を以下のノード（node）として抽出し、レイアウト的・意味的なつながりをエッジ（edge）で結ぶ： テキストチャンク 見出し 表 図 図・表のキャプション こうして文書を Document Graph に変換し、構造を保持したまま retrieval できる。 ポイント: 図の近くにある説明文 表とキャプション 見出しとそれに属する段落 これらをリンクで残すことで「構造を含む retrieval」が可能になる。 Graph-aware Retrieval ― 構造を考慮した検索 SuperRAG の retrieval は embedding 類似度だけでなく、以下の構造情報を使う： ノード同士のリンク（エッジ） 文書内の位置関係 ページ内の近接性 表や図の周辺テキスト そのため、たとえば次の問いにも対応できる： 「図2の結果と表3の値を比較して説明せよ」 「この表の数値が意味する内容を本文のどこで説明しているか」 「キャプションの意図を本文と統合して要約せよ」 従来 RAG でバラバラだった情報を、まとめて取り出せる。 実際の性能：SuperRAG はどれくらい強いのか？ Table 1: Document Reading Performance Table 1 は「文書を読み取る能力（Document Reading）」の評価。SuperRAG で文書グラフを作る前段として、PDF/画像からテキスト・図・表・位置・キャプションを正確に抽出できるかを測る。 指標: NID / TEDS / TEDS-S（レイアウトと表構造の再現性を評価） 比較: Amazon Textract, LlamaParse, Google, Azure DI, Our Reader（SuperRAG用） スコアの読みどころ（コメント要約）: Amazon Textract: 最高水準のNID、TEDS/TEDS-Sも高く安定。 LlamaParse: NID高いがTEDS/TEDS-Sは中程度で構造再現がやや不安定。 Google / Azure: 画像文書には強いが表関連では弱め。 Our Reader (SuperRAG用): Textractに肉薄する NID 92.43 / TEDS 87.19 で構造保持がバランス良好。表を含む TEDS-S でも高スコア。 つまり Our Reader は Textract級の構造復元力を持ち、後段のグラフ化・retrieval の土台が強い。 Table 2: QA Task Performance 技術資料・論文の QA で： Standard RAG: EM 30〜40%, F1 45〜55% SuperRAG: EM 40〜52%, F1 56〜68% F1 が +10〜+15 ポイント改善。図・表・段組みを含む文書理解に強いことがわかる。 Table 3: Multimodal Integration Task 図と本文、表と本文を統合して答える複合 QA の性能評価。 テキストのみモデル比で 30〜50% の相対改善 キャプションなしモデルより圧倒的に高い正答率 性能まとめ retrieval の質が向上 図表を含む複雑文書での QA が強化 F1 が大きく改善（+10〜15） マルチモーダル統合タスクで圧倒的 強みと弱み（実用目線） 強み 文書構造を保持：段落・図・表・キャプションをまとめて扱える マルチモーダル文書に強い：技術マニュアル／論文／報告書／PDF Retrieval の取りこぼしが減る：構造リンクで「近くの説明文」も拾う 弱み / 課題 レイアウト解析精度に依存：OCR が弱い言語では追加工夫が必要 グラフ構築と検索が重い：システムが大きくなりやすい スケール問題：大量 PDF ではグラフ生成コストが跳ね上がる まとめ SuperRAG は、従来 RAG の天敵だった 図 表 キャプション レイアウト構造 といった “文書の本来の構造” を保持し、retrieval・generation に反映する。実験結果でもチャンク型 RAGに対し上位互換的な性能を示す。技術文書やマニュアル、論文など、構造が命の文書を扱う場合には注目すべきアプローチだ。 参考文献 Chening Yang, Duy-Khanh Vu, Minh-Tien Nguyen, Xuan-Quang Nguyen, Linh Nguyen, Hung Le. SuperRAG: Beyond RAG with Layout-Aware Graph Modeling. NAACL Industry Track 2025. https://arxiv.org/abs/2503.04790 ACL Anthology version: https://aclanthology.org/2025.naacl-industry.45.pdf"
},

{
  "title": "よく使うコマンド集",
  "url": "/articles/commands/",
  "date": "2025.09.30",
  "description": "DockerやGitでよく使うコマンドを自分用メモとしてまとめました。",
  "content": "ここでは開発や環境構築で頻繁に使うコマンドをカテゴリ別にまとめています。必要に応じて随時追記します。 Docker / Docker Compose Docker Composeを起動・停止 docker compose up -d # バックグラウンドで起動 docker compose down # 停止・コンテナ削除 docker compose stop # 停止のみ コンテナ内のシェルに入る docker exec -it [コンテナ名またはID] bash # または sh ビルドキャッシュを使わずにイメージを再構築 docker build --no-cache . Git 最新のリモートブランチを取り込む git pull origin [ブランチ名] 変更をステージングに追加 git add . # すべて追加 git add [ファイル名] # 指定ファイルのみ 使い方メモ 新しいコマンドを見つけたらこのページに追記します。 よく忘れるオプションはコメントとして併記しておくと後で便利です。"
},

{
  "title": "BGE-M3：3つの検索手法を統合した新世代埋め込みモデル",
  "url": "/articles/bge-m3/bge-m3.html",
  "date": "2025.09.20",
  "description": "BGE-M3のアーキテクチャと学習戦略について包括的にまとめたメモ。",
  "content": "BGE-M3は、従来の単一的なベクトル検索（意味検索）の限界を超えるため、目的の異なる3つの検索手法を単一モデル内に統合することで、検索精度を飛躍的に向上させています。本稿では、その革新的なアーキテクチャと学習戦略について詳しく解説します。 はじめに：BGE-M3とは何か？ BGE-M3は、BAAI（北京智源人工智能研究院）によって開発された、検索タスクに特化した最先端のテキスト埋め込み（Embedding）モデルです。その最大の特徴は、M3パラダイムと呼ばれる3つの汎用性を、単一のモデルで実現した点にあります。 多言語性 (Multi-Linguality): 100以上の言語を単一モデルで処理。 多機能性 (Multi-Functionality): 目的の異なる3つの検索手法（Dense, Sparse, Multi-vector）を同時に実行。 多粒度性 (Multi-Granularity): 短い文から最大8192トークンの長文まで対応。 図1: 論文の図1にあるBGE-M3の全体アーキテクチャ ハイブリッド検索アーキテクチャ BGE-M3は、単一のテキストエンコーダを用いて、3種類の検索手法に対応する出力を同時に生成します。エンコーダが入力テキストを処理し、各トークンの文脈情報を保持した隠れ状態ベクトル（Hidden States）の集合を生成します。このベクトル集合を「共通の土台」として、各検索手法がそれぞれの方法で関連度スコアを計算します。 Dense Retrieval (密検索) テキスト全体の意味的な類似性を捉える手法です。エンコーダの出力のうち、特殊トークン[CLS]に対応する隠れ状態ベクトルを正規化し、テキスト全体を代表する単一の埋め込みベクトルとして使用します。 クエリ $q$ とパッセージ $p$ の関連度スコアは、それぞれの埋め込みベクトル $E_q$ と $E_p$ の内積によって計算されます。 $$Score_{dense}(q, p) = E_q \\cdot E_p$$ Lexical Retrieval (字句検索) テキスト内の各用語（トークン）の重要度を推定する手法です。クエリ内の各用語の重み $W_{q,i}$ は、対応する隠れ状態ベクトル $H_{q,i}$ と、学習によって得られる重み行列 $M_{lex}$ を用いて計算されます。 $$W_{q,i} = text{ReLU}(H_{q,i} M_{lex})$$ 最終的な関連度スコアは、クエリとパッセージの両方に共通して出現する用語について、それぞれの重みを掛け合わせた値を合計することで算出されます。 $$Score_{lex}(q, p) = sum_{i,j text{ s.t. } q_i=p_j} max(0, W_{q,i}) cdot max(0, W_{p,j})$$ Multi-Vector Retrieval (多ベクトル検索) テキストをトークンごとのベクトル集合として表現し、より精密な比較を行う手法です。エンコーダが出力した全ての隠れ状態ベクトルに対し、学習可能な射影行列 $M_{multi}$ を適用して変換します。 $$V_{q,i} = H_{q,i} M_{multi}$$ その後、ColBERTで提案されたLate Interaction（遅延相互作用）メカニズムに基づき、クエリの各トークンベクトルと、パッセージの全トークンベクトルとの間で最も類似度が高いものを探し、それらのスコアを平均して最終的な関連度スコアとします。 $$Score_{multi}(q, p) = frac{1}{|q|} sum_{i=1}^{|q|} max_{j=1}^{|p|} text{sim}(V_{q,i}, V_{p,j})$$ これら3つの手法を用いることで高い検索精度を達成しています。論文によると、各スコアを以下のように足し合わせます。 $$Score_{hybrid}(q, p) = lambda_1 Score_{dense} + lambda_2 Score_{lex} + lambda_3 Score_{multi}$$ 論文によると、$lambda_2$ と $lambda_3$ は少し下げているとのことです。 学習戦略 自己知識蒸留 (Self-Knowledge Distillation) 3つの異なる検索手法の学習目標は互いに競合しうるため、単純なマルチタスク学習では性能が低下する可能性があります。この問題を解決するため、BGE-M3は自己知識蒸留という独自の手法を提案します。 これは、3つの手法のスコアを統合して生成した、より賢明な「教師」スコアをモデル自身が作り出し、各手法がその教師から協調的に学習する仕組みです。 図2: 論文の図2にある自己知識蒸留のプロセス この手法の重要性は実験結果にも明確に表れており、自己知識蒸留を用いない場合、特にLexical Retrievalの性能が著しく低下（nDCG@10スコアが53.9→36.7）します。 実験と評価 BGE-M3の有効性を検証するため、多言語検索、クロスリンガル検索、長文検索の3つのタスクで広範な評価が行われました。 多言語検索: MIRACLベンチマークで既存モデルを大幅に上回る性能を示しました。 クロスリンガル検索: MKQAベンチマークでも高い性能を維持し、低リソース言語でも安定した性能を発揮します。 長文検索: MLDRベンチマークおよびNarrativeQAにおいて、ハイブリッド検索による著しい性能向上が見られました。特に長文タスクでは、Sparse Retrievalが重要な役割を果たしています。 Ablation Study (アブレーションスタディ) モデルの各要素の貢献度を調査するため、アブレーションスタディが実施されました。特に自己知識蒸留を除去すると、特にSparse Retrievalの性能が著しく低下し（53.9→36.7）、異なる手法間の目的の競合を緩和する上で自己知識蒸留が不可欠であることが示されました。この結果は、論文のTable 2で詳細に示されています。 図3: 論文のTable 5にあるアブレーションスタディの結果 結論 BGE-M3の卓越した検索精度は、単一の手法が優れているからではなく、自己知識蒸留によって3つの異なる検索アプローチを相乗効果が生まれる形で巧みに統合している点にあります。この革新的なアプローチは、テキスト埋め込みモデルの新たな可能性を示唆しています。 引用 Shitao Xiao, Peitian Zhang, Zheng Liu, Xingyu He, Shihan Dou, Yuxuan Wang, Yajing Xu, Huangcan Li, Chao Zhang, and Jiangui Chen. 2024. BGE M3: Embedding Everything in a Single Model. arXiv preprint arXiv:2402.03216."
},

{
  "title": "LightRAGとHNSWの技術解説メモ",
  "url": "/articles/LightRAG/LightRAG.html",
  "date": "2025.08.13",
  "description": "LightRAGとHNSWの仕組みとアーキテクチャを解説したメモ。",
  "content": "このメモは、次世代RAGフレームワーク「LightRAG」と、その中核をなす高速ベクトル検索技術「HNSW」の仕組みについて、個人的にまとめたものです。 1. LightRAGの全体像とアーキテクチャ 1.1. 従来のRAGが抱える課題とLightRAGの解決策 従来のRAG（検索拡張生成）は、情報を単なるテキストの塊（チャンク）として扱うため、以下のような課題がありました。 情報の断片化: 複数の文書にまたがる複雑な質問に対し、文脈が途切れた断片的な回答しか生成できない。 関係性の欠如: 「A社がB社を買収した」と「B社の創設者は田中氏」という2つの情報から、「A社が田中氏を迎え入れた」という関係性を導き出すことが困難。 LightRAGは、この課題をナレッジグラフを用いて解決します。テキストからエンティティ（ノード）と関係性（エッジ）を抽出し、「意味の地図」として構造化することで、深い文脈理解と一貫性のある回答生成を可能にします。 1.2. LightRAGのアーキテクチャ：2つの頭脳の連携 LightRAGは、役割の異なる2つの主要なデータストア（頭脳）を連携させて動作します。以下の図は、その全体構造を示しています。 図1: LightRAGの構造 ナレッジグラフ・ストア (Knowledge Graph Store) 役割: 「意味を理解する脳」として機能します。ノード（例: A社）、エッジ（例: 買収した）、そしてそれらの構造的なつながりを保持します。データ間の論理的な関係性を管理する役割を担います。 構造: このグラフ自体は階層化されておらず、単一でフラットなネットワーク構造です。 ベクトルデータベース (Vector Database) 役割: 「超高速な反射神経」として機能します。ナレッジグラフ内の各要素（ノードやエッジの説明文など）をベクトル化して保存し、高速な類似検索を実現します。 内蔵エンジン: このデータベースの内部には、検索を高速化するための索引（インデックス）技術としてHNSWが組み込まれています。 1.3. LightRAGの動作フロー①：ナレッジグラフの構築 LightRAGは、以下のステップで非構造化テキストからナレッジグラフを構築します。 抽出 (Extraction): LLMを使い、テキストからエンティティ（人物、組織など）と、それらの間の関係性を抽出します。 プロファイリング (Profiling): 抽出した各ノードとエッジに対し、LLMが検索用のキーワードや要約文を付与します。この情報がベクトル化され、ベクトルデータベースに登録されます。 重複排除 (Deduplication): 新しく抽出されたエンティティが既存のものと同一かを判定します。同一であれば、新しいノードは作らず、既存のノードに情報を統合します。この判定には、エンティティの正規化された名前などから生成されるハッシュ値がIDとして用いられると考えられます。 1.4. LightRAGの動作フロー②：デュアルレベル検索 ユーザーからの質問に答える際、LightRAGは独自のデュアルレベル検索パラダイムを用います。 低レベル検索 (Low-Level Retrieval): 「A社のCEOは誰？」のような、具体的なエンティティに関する質問に対応します。質問から「A社」「CEO」といったキーワードを抽出し、特定のノードをターゲットに検索を行います。 高レベル検索 (High-Level Retrieval): 「近年のAI業界の動向は？」のような、広範で抽象的なテーマに関する質問に対応します。「AI業界」「動向」といったキーワードから、複数のノードやエッジ（関係性）を横断的に検索し、全体的な文脈を捉えます。 この2つの検索は、ベクトルデータベースのメタデータ（例: type='node', type='edge'）を利用して効率的に（多くの場合、非同期で）実行され、得られた結果をナレッジグラフ上で統合し、最終的な回答を生成します。 2. 中核技術HNSW：高速検索の心臓部 HNSW (Hierarchical Navigable Small World) は、膨大なベクトルデータの中から、目的のベクトルに最も近いものを高速に見つけ出すためのアルゴリズムです。 2.1. 階層化の仕組み：ランダム性こそが鍵 HNSWの最大の特徴は、その巧妙な階層化の仕組みです。以下の図は、その階層構造を模式的に示しています。 図2: HNSWの階層構造 階層決定は完全にランダム: 新しいデータ点が追加されるたびに、その点がどの階層まで所属するかは、そのデータの内容や重要度とは一切関係なく、サイコロを振るようにランダムな確率で決まります。階層が上がるほど、その階層に所属する確率は指数関数的に低くなります。 階層構造の役割: このランダム性により、自然と効率的な階層構造が生まれます。 上位階層: メンバーが非常に少ないため、データ空間全体を横断する「高速道路」のような長距離リンクが形成されます。 下位階層 (0階): 全てのメンバーが所属するため、「一般道」のような短距離リンクが密集したネットワークが形成されます。 2.2. 探索の仕組みと新規データ追加 HNSWは、この階層構造を活かして、以下の手順で探索を行います。 最上位から開始: 探索は、最もまばらな最上位レイヤーの「エントリーポイント」から始まります。 貪欲な探索 (Greedy Traversal): 現在いるノードから、リンクで繋がっている隣人の中で、最もクエリ（探したいデータ）に近い隣人へと移動します。この計算には、テキストベクトルの場合、コサイン類似度が一般的に用いられます。 下の階層へ: そのレイヤーで「これ以上近づけない」という局所的な最適解に達したら、その地点を新しい開始点として、一つ下の、より密なレイヤーに降りて探索を続けます。 繰り返し: このプロセスを最下層（0階）まで繰り返すことで、最終的な検索結果を得ます。 また、HNSWは増分更新が可能で、新しいデータが追加された際も、この探索アルゴリズムを使って自身の最適な場所を見つけ出し、ネットワークにスムーズに統合されます。 3. 個人的に気になった疑問と回答 ここでは、私がHNSWに関して抱いた疑問とその回答をまとめます。 Q: HNSWの上位レイヤーが「コアな単語」や「マイナーな単語」だけで構成された場合、探索はうまくいきますか？ A: はい、問題ありません。階層への所属はランダムであり、単語の重要度には依存しません。上位レイヤーの役割は、データ空間全体にバランス良く「ショートカット網（高速道路）」を構築することです。そのため、構成要素が何であれ、ナビゲーション機能は頑健に働きます。 Q: 探索中に極所解に陥ることはありますか？ A: 極めて陥りにくい設計になっています。その理由は、①階層決定のランダム性、②トップダウンの探索戦略、そして次に説明する③賢い隣人選びのヒューリスティックという3つのメカニズムが連携して機能するためです。 Q: 「多様な方向にある隣人を選ぶヒューリスティック」とは？ A: これは、リンクの「数」ではなく「質」と「配置」を工夫するルールです。単純に最も近い隣人とだけリンクを結ぶと、すべてのリンクが同じデータクラスター内に集中し、探索が孤立する危険があります。このヒューリスティックは、たとえ少し距離が遠くても、自分から見て多様な方向にある隣人とバランス良くリンクを結ぶことを促します。これにより、クラスター間に意図的に「橋」が架けられ、探索が袋小路に入るのを防ぎます。 Q: 各ノードのIDはハッシュ値ですか？ A: 論文に直接の記載はありません。しかし、LightRAGの重要機能である「重複排除（Deduplication）」を実現するためには、IDが「決定的（同じエンティティからは必ず同じIDが生成される）」である必要があります。この要件を最も満たすのがハッシュ値です。エンティティの正規化された名前などからハッシュ値を計算し、それを一意なIDとして使用していると考えるのが最も合理的です。 4. 参考文献 LightRAG: Guo, Z., Xia, L., Yu, Y., Ao, T., & Huang, C. (2024). LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv preprint arXiv:2410.05779. HNSW: Malkov, Y. A., & Yashunin, D. A. (2018). Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4), 824-836."
},

{
  "title": "技術メモ：Transformerの位置符号（Positional Encoding）について",
  "url": "/articles/transformer-positional/transformer-positional.html",
  "date": "2025.08.13",
  "description": "Transformerの位置符号の重要性と計算方法を解説したメモ。",
  "content": "はじめに：本記事について Transformerモデルの内部で使用される位置符号（Positional Encoding）について、私が理解した内容をまとめます。特に、位置符号の計算方法やその背後にあるアイデアを深く掘り下げて解説します。 それぞれインプットに対して、位置符号をどのように計算するかを具体的に見ていきます。 図1: Transformerの構造 input encoding, output encodingの後の処理であるpositional encodingの部分の処理についてです。 なぜ位置符号が必要なのか？ Transformerモデルの心臓部であるSelf-Attentionは、文中の単語同士の関連度を計算する非常に強力な仕組みです。しかし、その計算過程で単語の元の順序が失われてしまうという大きな弱点があります。例えば、「犬が猫を追いかけた」と「猫が犬を追いかけた」では意味が全く異なりますが、Self-Attentionにとっては、単に「犬、猫、追いかけた、が、を」という単語の集合に見えてしまい、区別がつきません。 この問題を解決するのが位置符号（Positional Encoding）です。その目的は、入力される各単語のベクトルに対し、文章中におけるその単語の「住所」や「位置」に関する情報を、数学的に埋め込むことです。これにより、Transformerは単語の意味だけでなく、その順序も理解できるようになります。 第1章：位置符号の計算方法 1.1 計算式 位置符号は、文章中の単語の位置を $pos$ （0から始まる番号）、モデルが扱うベクトルの次元数を $d_{model}$ としたとき、ベクトルの各次元 $i$ に対して以下の式で計算されます。 偶数次元 ($2i$) の場合: $PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$ 奇数次元 ($2i+1$) の場合: $PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$ この計算によって、各位置 $pos$ ごとに、$d_{model}$ 次元のユニークなベクトル（位置符号ベクトル）が生成されます。最終的に、この位置符号ベクトルが、元の単語の意味を表すベクトル（単語埋め込み）に足し算されます。 1.2 具体例で学ぶ計算ステップ ($d_{model}=4$の場合) 文章「私は 猫が 好き」で、2番目の単語「猫」($pos=1$)の位置符号を計算してみましょう。 ベクトルの次元数 ($d_{model}$): 4次元 次元インデックス ($i$): $d_{model}/2 = 2$ なので、 $i=0$ と $i=1$ の2つのペアを考えます。 ① i=0のペア（0次元目, 1次元目）の計算 分母: $10000^{2 \\times 0 / 4} = 10000^0 = 1$ 0次元目(偶数): $PE_{(1, 0)} = \\sin(1/1) = \\sin(1) \\approx 0.841$ 1次元目(奇数): $PE_{(1, 1)} = \\cos(1/1) = \\cos(1) \\approx 0.540$ ② i=1のペア（2次元目, 3次元目）の計算 分母: $10000^{2 \\times 1 / 4} = 10000^{1/2} = 100$ 2次元目(偶数): $PE_{(1, 2)} = \\sin(1/100) \\approx 0.010$ 3次元目(奇数): $PE_{(1, 3)} = \\cos(1/100) \\approx 0.999$ ③ 結果の結合 「猫」($pos=1$)の位置符号ベクトルは、[0.841, 0.540, 0.010, 0.999]となります。同様に、他の位置の単語も全く異なるベクトルが生成され、位置を区別できます。 第2章：なぜこの仕組みなのか？ 核心概念の深掘り ここからは、対話の中で生まれた疑問とその答えを通して、この数式の裏にある巧みなアイデアを解説します。 2.1 なぜ次元ごとに波を変えるのか？ - インデックスiの役割 【疑問】 各次元のインデックス $i$ は何を意味しているのか？ $i$ は、位置符号ベクトルを構成する波のパターン（周波数）を変えるための「スイッチ」の役割を担っています。 計算式の分母 $10000^{2i/d_{model}}$ に注目すると、$i$ が小さいほど分母は小さく（1に近く）、$i$ が大きいほど分母は大きくなります。これは、sin/cosの中身に影響を与え、結果として波の揺れ方を変えます。 $i$ が小さい次元（ベクトルの前半）: 波の揺れがゆっくりになります（低周波）。これは、文章全体における大局的な位置関係を捉えるのに役立ちます。4次元の例では、0次元目と1次元目がこれにあたります。 $i$ が大きい次元（ベクトルの後半）: 波の揺れが速くなります（高周波）。これは、隣り合う単語同士の細かい、局所的な位置関係を捉えるのに役立ちます。4次元の例では、2次元目と3次元目がこれにあたります。 これは、様々な楽器が重なって一つの楽曲を作るオーケストラに似ています。低音担当の楽器（$i$ が小さい）と高音担当の楽器（$i$ が大きい）がそれぞれのメロディーを奏でることで、豊かで複雑なハーモニー（位置情報）が生まれるのです。 2.2 なぜsinとcosをペアにするのか？ - 「回転」という最強のアイデア 【疑問】 ペアにすることと「回転」の関係性がわからない。なぜペアにすると回転できるのか？ この疑問の答えこそ、位置符号の最もエレガントな部分です。 ペアで「円」を作る: もし1次元、例えば $\\sin(pos)$ だけで位置を表そうとすると、その値は数直線上を行ったり来たりするだけで、向きが一定しません。しかし、$\\cos(pos)$ と $\\sin(pos)$ をペアで使うと、これは2次元平面上の円周上の点の座標 (x, y) を表現することになります。$pos$ が増えることは、この円周上を点がクルクルと回転することを意味します。 回転で「相対位置」を測る: この「回転」という性質のおかげで、ある位置 $pos$ から「k個隣」の位置 $pos+k$ への移動は、どの $pos$ から出発しても、常に同じ角度だけの回転操作として表現できます。この回転操作は、数学的には回転行列 $R_k$ という「物差し」で表せます。 【理解の核心】 モデルが学習するのは、「位置2から位置3への行き方」や「位置8から位置9への行き方」といった無数の個別ケースではありません。モデルは、「\"1つ隣\"という関係性は、この $R_1$ という物差し（回転操作）を使えば測れる」という、たった一つの普遍的なルールを学習すれば良いのです。 これにより、モデルは文の長さや絶対位置に依存しない、非常に効率的で強力な「相対位置を認識する能力」を獲得します。 2.3 複数ペアはどう動くのか？ - 独立した世界での並列処理 【疑問】 4次元ベクトルの場合、2ペア分の回転ベクトルを「加える」ことで表現するのか？ これは非常に鋭い疑問です。答えは「加える」のではなく、「各ペアが独立して回転し、その結果を並べて結合（Concatenate）する」です。 4次元ベクトルは、2つの独立した「2次元の世界」だと考えます。 世界A ($i=0$ のペア): 大局的な位置を測る、周期の長い世界。 世界B ($i=1$ のペア): 局所的な位置を測る、周期の短い世界。 「k個隣」への移動を計算するとき、 世界Aの中で、専用の物差し $R_k(i=0)$ を使って回転が起こる。 それと並行して、全く独立に、世界Bの中でも専用の物差し $R_k(i=1)$ を使って回転が起こる。 最終的な4次元ベクトルは、世界Aの回転後の座標と、世界Bの回転後の座標を、単純に横に並べて作られます。 この「並列・独立処理」により、大局的な位置情報と局所的な位置情報の両方を失うことなく、4次元のベクトルとして保つことができるのです。 第3章：他の概念との関連と補足 3.1 フーリエ変換との関係 - 「思想の継承」 【疑問】 この仕組みはフーリエ変換に似ているか？ はい、これは非常に的確な洞察です。位置符号はフーリエ変換そのものではありませんが、その根底にある「どんな複雑な情報も、単純な波（sin/cos）の組み合わせで表現できる」という思想を色濃く受け継いでいます。 フーリエ変換が既存の信号を分析して周波数成分に分解するのに対し、位置符号は位置という情報を表現するために、様々な周波数の波を組み合わせてベクトルを人工的に生成する、という違いはありますが、その発想は共通しています。 3.2 「10000」という数字の謎 【疑問】 式に出てくる「10000」はどこから来たのか？マジックナンバーか？ はい、これは一種のマジックナンバーであり、Transformerの原論文の著者らが経験的に設定したハイパーパラメータです。この数字は、位置符号が表現できる波の周期（波長）の範囲を決定します。この値が大きすぎず小さすぎず、様々な長さの文章に対応できるバランスの良い値として選ばれ、現在では事実上の標準となっています。 おわりに 個人的な解釈のまとめが以下です。 「1番目のトークンと2番目のトークンのi=0のペアは同じ世界にいて、その中の物差しで自由にどのトークンも表せられる」 この一文は、位置符号の仕組みの神髄を完璧に捉えています。 共通の世界: 全ての単語は、次元ペア $i$ ごとに作られる「同じ座標平面」上にプロットされる。 共通の物差し: その世界の中では、相対距離を測るための「共通の回転行列（物差し）」が使い回される。 自由な表現: これにより、どの単語を基準にしても、他の全ての単語との相対的な位置関係を、普遍的なルールで自由に表現できる。 このエレガントな仕組みこそが、Transformerが複雑な文章構造を理解し、驚異的な性能を発揮するための根幹の一つとなっているのです。 参考文献 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In *Advances in neural information processing systems* (pp. 5998-6008)."
},

{
  "title": "Docker + GitLabで出る「invalid spec: :/tmp/ssh-agent.sock」エラーの原因と解決法",
  "url": "/articles/Gitlab-ssh-error/Gitlab-ssh-error.html",
  "date": "2025.05.29",
  "description": "Docker + GitLabで発生する ssh-agent.sock エラーの原因と解決法。",
  "content": "GitLab 上のプロジェクトを Docker コンテナの中からクローンやプッシュなどの操作を行う際、SSH 認証が必要になります。通常ローカルでは、ホスト側の SSH エージェント（ssh-agent）が秘密鍵を保持しており、そのソケットファイルへのパスが **SSH_AUTH_SOCK** です。 このソケットを Docker コンテナ内にも共有することで、Docker 内からもホストの SSH 認証情報を使って GitLab にアクセスできるようになります。 🔍 エラーの正体は？ GitLab リポジトリを Docker コンテナから操作しようとした際、以下のようなエラーが出ることがあります。 invalid spec: :/tmp/ssh-agent.sock: empty section between colons これは、Docker Compose で SSH エージェントのソケットをボリュームとしてマウントしようとした際に、環境変数 **SSH_AUTH_SOCK** が未設定だったために発生します。 📦 なぜ SSH_AUTH_SOCK が必要なのか？ Docker コンテナから GitLab（SSH接続）にアクセスするには、ホストマシンで動作している ssh-agent を通じて認証情報（秘密鍵）を使えるようにする必要があります。このとき、**SSH_AUTH_SOCK** がそのエージェントのソケットファイルの場所を指しています。 ⚠️ エラーの発生パターン Docker Compose などで以下のような指定をしている場合を考えます。 volumes: - ${SSH_AUTH_SOCK}:/tmp/ssh-agent.sock しかし、ホスト側で **SSH_AUTH_SOCK** が未設定（または空文字）だった場合、Docker Compose は次のように解釈します。 :/tmp/ssh-agent.sock これは「ホスト側パス」が空なので、Docker は不正なボリューム定義と判断してエラーを出力します。 ✅ 解決法：SSH_AUTH_SOCK を正しく設定する 以下のコマンドを実行して、**ssh-agent** を起動・設定します。 # 1. ssh-agent を起動\" eval \"$(ssh-agent -s)\" これは SSH 認証エージェントのデーモンを起動し、バックグラウンドで秘密鍵を保持してくれます。このコマンドにより **SSH_AUTH_SOCK** という変数に、ソケットファイルのパスが自動的にセットされます。 例: # 2. SSH_AUTH_SOCK を設定 export SSH_AUTH_SOCK=$(find /tmp/ssh-* -type s) # 3. 環境変数が正しく設定されたか確認 echo $SSH_AUTH_SOCK # => /tmp/ssh-abc123/agent.5835 その後、Docker Compose を再実行します。 これにより、**docker-compose.yml** の以下の指定が、次のように展開されます。 volumes: - ${SSH_AUTH_SOCK}:/tmp/ssh-agent.sock 展開後: /tmp/ssh-xyz123/agent.1234:/tmp/ssh-agent.sock つまり、「ホスト → コンテナ」間で SSH 認証情報の橋渡しができるようになります。結果として、Docker コンテナ内から GitLab への SSH アクセスが成功します。 💡 補足 この方法は Dockerfileを一切変更せず、環境変数だけで安全に対処できるため、CI/CD や本番環境でも安定して利用可能です。"
},

{
  "title": "Docker + Poetryで始めるPython開発環境構築",
  "url": "/articles/docler-poetry/docker-poetry.html",
  "date": "2025.05.28",
  "description": "Docker + PoetryでPython開発環境を構築する手順メモ。",
  "content": "Pythonプロジェクトの開発環境を整える際、依存関係の管理や環境の再現性は常に課題となります。 本記事では、この課題を解決するための強力な組み合わせ、**Docker**と**Poetry**を利用したPython開発環境の構築方法について解説します。 これにより、プロジェクトごとにクリーンで再現性の高い環境を簡単に構築し、開発を効率的に進めることができるようになります。 なぜDockerとPoetryを使うのか？ Dockerはアプリケーションをコンテナとして隔離し、環境依存の問題を解消します。これにより「私の環境では動くのに…」といった問題をなくし、チーム開発やデプロイをスムーズにします。 一方、PoetryはPythonプロジェクトの依存関係管理とパッケージングを強力にサポートします。仮想環境の作成から依存ライブラリのインストール、スクリプト実行まで一元的に管理できるため、pipやvenvといった既存のツールに比べて格段に使いやすさが向上します。 この二つを組み合わせることで、OSや既存のPython環境に影響を与えることなく、プロジェクトに最適な開発環境を構築できます。 環境構築のファイル構成 まずは、今回構築する環境のファイル構成を見てみましょう。 . ├── .devcontainer/ │ └── devcontainer.json ├── .vscode/ │ └── settings.json ├── app/ │ └── main.py ├── docker-compose.yml ├── Dockerfile ├── poetry.lock (初回起動時に自動生成) ├── pyproject.toml └── README.md この構成で、コンテナの定義、Pythonプロジェクトの設定、VS Codeの統合設定までをカバーしています。 Dockerfile: コンテナの設計図 `Dockerfile`は、Python 3.12をベースに、Poetry、必要なシステムライブラリ、そして開発に必要なPythonライブラリをインストールする手順を定義します。 ここでは、`matplotlib`、`numpy`、`torch`、`japanize-matplotlib`、そしてVS CodeでJupyter Notebookを利用するためのライブラリを事前にインストールしています。 FROM python:3.12-slim-bookworm # タイムゾーンの設定 (任意) ENV TZ=Asia/Tokyo RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Poetryのインストール RUN pip install poetry==1.8.2 # 作業ディレクトリの設定 WORKDIR /app # Poetryのパス設定 (PATHに追加) ENV PATH=\"/root/.local/bin:$PATH\" # 依存関係のキャッシュを効率化するために、pyproject.tomlとpoetry.lockを先にコピー COPY pyproject.toml poetry.lock* ./ # 必要なシステムライブラリをインストール # matplotlib, numpy, torch, japanize-matplotlib, Jupyter Notebook関連 RUN apt-get update && apt-get install -y \\ build-essential \\ git \\ libffi-dev \\ libssl-dev \\ zlib1g-dev \\ libjpeg-dev \\ && rm -rf /var/lib/apt/lists/* # Poetryの依存関係をインストール # ここでJupyter Notebook関連のライブラリもインストールされます RUN poetry install --no-root --no-interaction --no-ansi # アプリケーションコードのコピー COPY . . # コンテナ起動時のデフォルトコマンド (任意) # CMD [\"bash\"] コード1: Dockerfileの内容 pyproject.toml: プロジェクトの依存関係と設定 `pyproject.toml`はPoetryがプロジェクトの依存関係を管理するための中心的なファイルです。 今回は、前述のライブラリに加えて、Jupyter Notebook関連のライブラリも追加しています。 PyTorchについては、CPU版を想定したソースを指定しています。GPU版を使用する場合は、適宜変更してください。 [tool.poetry] name = \"my-python-project\" version = \"0.1.0\" description = \"\" authors = [\"Your Name &lt;you@example.com&gt;\"] readme = \"README.md\" [tool.poetry.dependencies] python = \"^3.12\" matplotlib = \"^3.8.4\" numpy = \"^1.26.4\" torch = {version = \"^2.2.2\", source = \"pytorch\"} japanize-matplotlib = \"^1.1.3\" ipykernel = \"^6.29.4\" jupyter = \"^1.0.0\" notebook = \"^7.1.3\" ipython = \"^8.23.0\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" [[tool.poetry.source]] name = \"pytorch\" url = \"https://download.pytorch.org/whl/cpu\" priority = \"explicit\" コード2: pyproject.tomlの内容 docker-compose.yml: サービス定義と起動設定 `docker-compose.yml`は、Dockerコンテナをまとめて管理するためのファイルです。 `Dockerfile`で定義したイメージを元に`app`サービスを構築し、ホストOSとコンテナ間でファイルを共有（ボリュームマウント）、Jupyter Notebook用にポートを公開しています。 version: '3.8' services: app: build: context: . dockerfile: Dockerfile volumes: - .:/app ports: - \"8888:8888\" # Jupyter Notebook用 working_dir: /app tty: true # コンテナ内でbashを継続させるために必要 コード3: docker-compose.ymlの内容 VS Code連携のための設定ファイル VS Codeの「Remote - Containers」拡張機能を使うと、コンテナ内部を直接開発環境として利用できます。 `.devcontainer/devcontainer.json`と`.vscode/settings.json`は、この連携をスムーズにするための設定です。 特に`devcontainer.json`では、コンテナ起動後に自動で依存関係をインストールする設定などを記述しています。 // .devcontainer/devcontainer.json { \"name\": \"Python 3.12 with Poetry\", \"dockerComposeFile\": \"../docker-compose.yml\", \"service\": \"app\", \"workspaceFolder\": \"/app\", \"customizations\": { \"vscode\": { \"extensions\": [ \"ms-python.python\", \"ms-toolsai.jupyter\" ], \"settings\": { \"python.defaultInterpreterPath\": \"/root/.local/share/pypoetry/venv/bin/python\", \"python.terminal.activateEnvironment\": true, \"jupyter.notebookFileRoot\": \"${workspaceFolder}\" } } }, \"postStartCommand\": \"poetry install --no-root\", \"remoteUser\": \"root\" } コード4: .devcontainer/devcontainer.jsonの内容 // .vscode/settings.json { \"python.languageServer\": \"Pylance\", \"python.analysis.autoImportCompletions\": true, \"python.terminal.activateEnvironment\": true, \"jupyter.notebookFileRoot\": \"${workspaceFolder}\" } コード5: .vscode/settings.jsonの内容 環境構築と使い方 ファイルの準備ができたら、いよいよ環境を構築し、使ってみましょう。 1. Docker Composeで環境を起動 プロジェクトのルートディレクトリで以下のコマンドを実行します。初回はイメージのビルドに時間がかかります。 docker compose up --build -d これにより、バックグラウンドでコンテナが起動します。 2. コンテナに入る 起動したコンテナ内で作業を行うには、以下のコマンドでシェルに入ります。 docker compose exec app bash Dockerコンテナのシェル内でできること `docker compose exec app bash` コマンドでコンテナのシェルに入った後、通常のLinux環境と同様に様々な操作が可能です。 **ファイル操作**: `ls`, `cd`, `pwd`, `cp`, `mv`, `rm`, `mkdir` など、基本的なファイル・ディレクトリ操作コマンドが利用できます。 **テキストエディタ**: `vi` や `nano` (Dockerfileでインストールしていれば) を使ってファイルを直接編集できます。 **プロセスの確認**: `ps aux` で現在実行中のプロセスを確認できます。 **ネットワークコマンド**: `ping`, `curl`, `wget` などで外部との通信を確認したり、リソースをダウンロードしたりできます。 **システム情報の確認**: `df -h` (ディスク使用量), `free -h` (メモリ使用量) などでコンテナのシステム状況を確認できます。 **Poetryコマンドの実行**: `poetry env list`: 作成された仮想環境のリストを表示します。 `poetry run python -c \"import sys; print(sys.version)\"`: コンテナ内のPythonバージョンを確認できます。 `poetry run &lt;コマンド&gt;`: Poetryが管理する仮想環境内で任意のコマンドを実行します。 **Pythonインタプリタの起動**: `poetry run python` でPythonの対話型シェルを起動し、コードを試すことができます。 **Jupyter Notebookの起動 (オプション)**: `docker-compose.yml`でポートを公開している場合、コンテナ内でJupyter Notebookを起動し、ホストのブラウザからアクセスできます。 poetry run jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser 上記コマンド実行後、表示されるURL（トークン付き）をホストのブラウザで開くと、Jupyter Labにアクセスできます。 これらの操作を通じて、コンテナ内部の環境を探索し、デバッグや追加設定を行うことができます。 3. Poetryコマンドの利用 コンテナ内ではPoetryが利用可能です。 **新しいライブラリの追加**: `poetry add &lt;パッケージ名&gt;` **依存関係のインストール**: `poetry install` (pyproject.tomlに基づいて依存関係をインストール) **スクリプトの実行**: `poetry run python app/main.py` 4. VS Code Remote - Containersでの開発 最もおすすめの開発方法です。VS Codeに「Remote - Containers」拡張機能をインストールした後、以下の手順で開発を開始できます。 VS Codeを開き、コマンドパレット (`Ctrl+Shift+P` または `Cmd+Shift+P`) を開き、「Remote-Containers: Open Folder in Container...」を選択します。 このプロジェクトのルートディレクトリを選択します。 VS Codeが自動的にコンテナに接続し、その中で開発環境を開きます。ターミナルもコンテナ内につながり、Jupyter Notebookもシームレスに利用できます。 サンプルコード `app/main.py`に以下のサンプルコードを配置することで、`matplotlib`や`japanize-matplotlib`、`numpy`、`torch`が正しく動作するかを確認できます。 import matplotlib.pyplot as plt import japanize_matplotlib import numpy as np import torch def main(): print(\"Hello from Dockerized Poetry environment!\") print(f\"NumPy version: {np.__version__}\") print(f\"Torch version: {torch.__version__}\") # Matplotlibとjapanize-matplotlibのテスト x = np.linspace(0, 2 * np.pi, 100) y = np.sin(x) plt.figure(figsize=(8, 6)) plt.plot(x, y) plt.title(\"正弦波のグラフ（日本語対応）\") plt.xlabel(\"X軸\") plt.ylabel(\"Y軸\") plt.grid(True) # plt.show() # VS CodeのJupyterなどで実行する場合はコメントアウト if __name__ == \"__main__\": main() コード6: app/main.pyのサンプルコード `plt.show()`はGUI環境が必要なため、Jupyter Notebookやインタラクティブシェルで実行する場合はコメントアウトまたは適切に処理してください。 まとめと今後の展望 DockerとPoetryを組み合わせることで、Python開発の環境構築が格段に効率化され、プロジェクトの再現性と保守性が向上します。 特にVS CodeのRemote - Containers機能との連携は、開発体験を大きく向上させるでしょう。 このテンプレートを基に、皆さんのPythonプロジェクトがよりスムーズに進むことを願っています。 ご質問やさらなるカスタマイズのご要望があれば、お気軽にご連絡ください。"
},

{
  "title": "成人式の思い出と活動について",
  "url": "/articles/coming_of_age/coming_of_age.html",
  "date": "2021.01.10",
  "description": "成人式での活動や当時の思い出をまとめた記事。",
  "content": "2021年、新型コロナウイルスの影響で、私が住む越谷市では成人式が中止となってしまいました。 一生に一度の成人式が失われることに寂しさを感じ、何かできることはないかと考えていたとき、 元越谷市立西中学校のサッカー部のコーチや先生方と共に**「越谷ミレニアム&+1成人式@埼玉スタジアム」**を企画する機会を得ました。 活動概要 **Date**: 2021年2月28日 **Location**: 埼玉スタジアム2002 **Background**: 新型コロナウイルス感染症の影響による越谷市の成人式中止を受け、同級生と共に成人式を再企画。 **Role**: 実行委員長に声をかけられ、**書記・映像プロデューサー**として活動しました。 具体的には、会議での資料まとめ、ティザームービー作成、当日のオーロラビジョンでのスライドや映像の作成、 頂いた映像の編集、音響の調節などを担当しました。 **Funding**: クラウドファンディングにて**160万円**の資金を集めることに成功しました。 **Guest Messages**: 元KRUSH -65キロ級王者 HIROYAさん、元WBAスーパーフェザー級王者 山内 高志さん、 COLOR,DEEP,LUVANDSOUL KIKURIさん、DEEPリーダー TAKAさんからお祝いのメッセージを頂きました。 **Media Coverage**: 東部読売新聞に2回掲載されたほか、当日はNHKの取材が行われ、NHK首都圏ニュースにて放送されました。 活動を振り返って 困難な状況の中でも、仲間たちと協力して成人式を企画し、多くの人々の支援を得られたことは、 私にとってかけがえのない経験となりました。 特に、映像制作を通じてイベント全体の盛り上げに貢献できたこと、そして当日の参加者の笑顔を見たときの達成感は忘れられません。 この経験は、目標に向かってチームで協力することの重要性や、困難な状況を乗り越える力を私に与えてくれました。 以下に、当時の活動の様子を伝えるメディア掲載の画像や映像を掲載します。"
}

]
