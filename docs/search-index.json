[

{
  "title": "SuperRAG: 文書レイアウトを理解する次世代 RAG の仕組みと性能を徹底解説する",
  "url": "/articles/superRAG/",
  "date": "2025.12.10",
  "description": "文書のレイアウト構造ごと理解する「SuperRAG」の仕組みと性能を、図表付きで解説します。",
  "content": "SuperRAG: 文書レイアウトを理解する次世代 RAG の仕組みと性能を徹底解説する RAG（Retrieval-Augmented Generation）は、技術文書やマニュアル、研究資料の検索・QA に広く使われている。しかし従来の RAG は、文書を 「テキストチャンクの集合」 として扱うことが前提で、図や表、段組み、キャプションなどを一切考慮しない。そのため本文と図表の関係性を理解できず、重要な情報が落ちる。 論文 “SuperRAG: Beyond RAG with Layout-Aware Graph Modeling” は、文書をレイアウト構造ごと理解する RAG を提案する。本記事では、仕組みからベンチマーク結果までを図表付きで整理する。 従来 RAG の問題点 ― なぜ「構造」が必要なのか 従来の流れ： 文書を一定長でチャンク分割 チャンク単位でベクトル化 類似度の高いチャンクを検索 LLM に渡して回答生成 この方式の弱点： 図とキャプションが分離される 表（テーブル）と説明段落が離れる 段組み・上下関係などのレイアウト構造が消える 「図2の情報を基に表3の値と比較する」ような統合理解ができない 技術文書・論文・マニュアルは複雑な構造があるからこそ情報が正確に伝わる。そこを壊す従来 RAG では限界がある。 SuperRAG の中核アイデア：文書を “グラフ” として扱う SuperRAG は、文書を以下のノード（node）として抽出し、レイアウト的・意味的なつながりをエッジ（edge）で結ぶ： テキストチャンク 見出し 表 図 図・表のキャプション こうして文書を Document Graph に変換し、構造を保持したまま retrieval できる。 ポイント: 図の近くにある説明文 表とキャプション 見出しとそれに属する段落 これらをリンクで残すことで「構造を含む retrieval」が可能になる。 Graph-aware Retrieval ― 構造を考慮した検索 SuperRAG の retrieval は embedding 類似度だけでなく、以下の構造情報を使う： ノード同士のリンク（エッジ） 文書内の位置関係 ページ内の近接性 表や図の周辺テキスト そのため、たとえば次の問いにも対応できる： 「図2の結果と表3の値を比較して説明せよ」 「この表の数値が意味する内容を本文のどこで説明しているか」 「キャプションの意図を本文と統合して要約せよ」 従来 RAG でバラバラだった情報を、まとめて取り出せる。 実際の性能：SuperRAG はどれくらい強いのか？ Table 1: Document Reading Performance Table 1 は「文書を読み取る能力（Document Reading）」の評価。SuperRAG で文書グラフを作る前段として、PDF/画像からテキスト・図・表・位置・キャプションを正確に抽出できるかを測る。 指標: NID / TEDS / TEDS-S（レイアウトと表構造の再現性を評価） 比較: Amazon Textract, LlamaParse, Google, Azure DI, Our Reader（SuperRAG用） スコアの読みどころ（コメント要約）: Amazon Textract: 最高水準のNID、TEDS/TEDS-Sも高く安定。 LlamaParse: NID高いがTEDS/TEDS-Sは中程度で構造再現がやや不安定。 Google / Azure: 画像文書には強いが表関連では弱め。 Our Reader (SuperRAG用): Textractに肉薄する NID 92.43 / TEDS 87.19 で構造保持がバランス良好。表を含む TEDS-S でも高スコア。 つまり Our Reader は Textract級の構造復元力を持ち、後段のグラフ化・retrieval の土台が強い。 Table 2: QA Task Performance 技術資料・論文の QA で： Standard RAG: EM 30〜40%, F1 45〜55% SuperRAG: EM 40〜52%, F1 56〜68% F1 が +10〜+15 ポイント改善。図・表・段組みを含む文書理解に強いことがわかる。 Table 3: Multimodal Integration Task 図と本文、表と本文を統合して答える複合 QA の性能評価。 テキストのみモデル比で 30〜50% の相対改善 キャプションなしモデルより圧倒的に高い正答率 性能まとめ retrieval の質が向上 図表を含む複雑文書での QA が強化 F1 が大きく改善（+10〜15） マルチモーダル統合タスクで圧倒的 強みと弱み（実用目線） 強み 文書構造を保持：段落・図・表・キャプションをまとめて扱える マルチモーダル文書に強い：技術マニュアル／論文／報告書／PDF Retrieval の取りこぼしが減る：構造リンクで「近くの説明文」も拾う 弱み / 課題 レイアウト解析精度に依存：OCR が弱い言語では追加工夫が必要 グラフ構築と検索が重い：システムが大きくなりやすい スケール問題：大量 PDF ではグラフ生成コストが跳ね上がる まとめ SuperRAG は、従来 RAG の天敵だった 図 表 キャプション レイアウト構造 といった “文書の本来の構造” を保持し、retrieval・generation に反映する。実験結果でもチャンク型 RAGに対し上位互換的な性能を示す。技術文書やマニュアル、論文など、構造が命の文書を扱う場合には注目すべきアプローチだ。 参考文献 Chening Yang, Duy-Khanh Vu, Minh-Tien Nguyen, Xuan-Quang Nguyen, Linh Nguyen, Hung Le. SuperRAG: Beyond RAG with Layout-Aware Graph Modeling. NAACL Industry Track 2025. https://arxiv.org/abs/2503.04790 ACL Anthology version: https://aclanthology.org/2025.naacl-industry.45.pdf"
},

{
  "title": "よく使うコマンド集",
  "url": "/articles/commands/",
  "date": "2025.09.30",
  "description": "DockerやGitでよく使うコマンドを自分用メモとしてまとめました。",
  "content": "ここでは開発や環境構築で頻繁に使うコマンドをカテゴリ別にまとめています。必要に応じて随時追記します。 Docker / Docker Compose Docker Composeを起動・停止 docker compose up -d # バックグラウンドで起動 docker compose down # 停止・コンテナ削除 docker compose stop # 停止のみ コンテナ内のシェルに入る docker exec -it [コンテナ名またはID] bash # または sh ビルドキャッシュを使わずにイメージを再構築 docker build --no-cache . Git 最新のリモートブランチを取り込む git pull origin [ブランチ名] 変更をステージングに追加 git add . # すべて追加 git add [ファイル名] # 指定ファイルのみ 使い方メモ 新しいコマンドを見つけたらこのページに追記します。 よく忘れるオプションはコメントとして併記しておくと後で便利です。"
},

{
  "title": "BGE-M3：3つの検索手法を統合した新世代埋め込みモデル",
  "url": "/articles/bge-m3/bge-m3.html",
  "date": "2025.09.20",
  "description": "BGE-M3のアーキテクチャと学習戦略について包括的にまとめたメモ。",
  "content": "BGE-M3は、従来の単一的なベクトル検索（意味検索）の限界を超えるため、目的の異なる3つの検索手法を単一モデル内に統合することで、検索精度を飛躍的に向上させています。本稿では、その革新的なアーキテクチャと学習戦略について詳しく解説します。 はじめに：BGE-M3とは何か？ BGE-M3は、BAAI（北京智源人工智能研究院）によって開発された、検索タスクに特化した最先端のテキスト埋め込み（Embedding）モデルです。その最大の特徴は、M3パラダイムと呼ばれる3つの汎用性を、単一のモデルで実現した点にあります。 多言語性 (Multi-Linguality): 100以上の言語を単一モデルで処理。 多機能性 (Multi-Functionality): 目的の異なる3つの検索手法（Dense, Sparse, Multi-vector）を同時に実行。 多粒度性 (Multi-Granularity): 短い文から最大8192トークンの長文まで対応。 図1: 論文の図1にあるBGE-M3の全体アーキテクチャ ハイブリッド検索アーキテクチャ BGE-M3は、単一のテキストエンコーダを用いて、3種類の検索手法に対応する出力を同時に生成します。エンコーダが入力テキストを処理し、各トークンの文脈情報を保持した隠れ状態ベクトル（Hidden States）の集合を生成します。このベクトル集合を「共通の土台」として、各検索手法がそれぞれの方法で関連度スコアを計算します。 Dense Retrieval (密検索) テキスト全体の意味的な類似性を捉える手法です。エンコーダの出力のうち、特殊トークン[CLS]に対応する隠れ状態ベクトルを正規化し、テキスト全体を代表する単一の埋め込みベクトルとして使用します。 クエリ $q$ とパッセージ $p$ の関連度スコアは、それぞれの埋め込みベクトル $E_q$ と $E_p$ の内積によって計算されます。 $$Score_{dense}(q, p) = E_q \\cdot E_p$$ Lexical Retrieval (字句検索) テキスト内の各用語（トークン）の重要度を推定する手法です。クエリ内の各用語の重み $W_{q,i}$ は、対応する隠れ状態ベクトル $H_{q,i}$ と、学習によって得られる重み行列 $M_{lex}$ を用いて計算されます。 $$W_{q,i} = text{ReLU}(H_{q,i} M_{lex})$$ 最終的な関連度スコアは、クエリとパッセージの両方に共通して出現する用語について、それぞれの重みを掛け合わせた値を合計することで算出されます。 $$Score_{lex}(q, p) = sum_{i,j text{ s.t. } q_i=p_j} max(0, W_{q,i}) cdot max(0, W_{p,j})$$ Multi-Vector Retrieval (多ベクトル検索) テキストをトークンごとのベクトル集合として表現し、より精密な比較を行う手法です。エンコーダが出力した全ての隠れ状態ベクトルに対し、学習可能な射影行列 $M_{multi}$ を適用して変換します。 $$V_{q,i} = H_{q,i} M_{multi}$$ その後、ColBERTで提案されたLate Interaction（遅延相互作用）メカニズムに基づき、クエリの各トークンベクトルと、パッセージの全トークンベクトルとの間で最も類似度が高いものを探し、それらのスコアを平均して最終的な関連度スコアとします。 $$Score_{multi}(q, p) = frac{1}{|q|} sum_{i=1}^{|q|} max_{j=1}^{|p|} text{sim}(V_{q,i}, V_{p,j})$$ これら3つの手法を用いることで高い検索精度を達成しています。論文によると、各スコアを以下のように足し合わせます。 $$Score_{hybrid}(q, p) = lambda_1 Score_{dense} + lambda_2 Score_{lex} + lambda_3 Score_{multi}$$ 論文によると、$lambda_2$ と $lambda_3$ は少し下げているとのことです。 学習戦略 自己知識蒸留 (Self-Knowledge Distillation) 3つの異なる検索手法の学習目標は互いに競合しうるため、単純なマルチタスク学習では性能が低下する可能性があります。この問題を解決するため、BGE-M3は自己知識蒸留という独自の手法を提案します。 これは、3つの手法のスコアを統合して生成した、より賢明な「教師」スコアをモデル自身が作り出し、各手法がその教師から協調的に学習する仕組みです。 図2: 論文の図2にある自己知識蒸留のプロセス この手法の重要性は実験結果にも明確に表れており、自己知識蒸留を用いない場合、特にLexical Retrievalの性能が著しく低下（nDCG@10スコアが53.9→36.7）します。 実験と評価 BGE-M3の有効性を検証するため、多言語検索、クロスリンガル検索、長文検索の3つのタスクで広範な評価が行われました。 多言語検索: MIRACLベンチマークで既存モデルを大幅に上回る性能を示しました。 クロスリンガル検索: MKQAベンチマークでも高い性能を維持し、低リソース言語でも安定した性能を発揮します。 長文検索: MLDRベンチマークおよびNarrativeQAにおいて、ハイブリッド検索による著しい性能向上が見られました。特に長文タスクでは、Sparse Retrievalが重要な役割を果たしています。 Ablation Study (アブレーションスタディ) モデルの各要素の貢献度を調査するため、アブレーションスタディが実施されました。特に自己知識蒸留を除去すると、特にSparse Retrievalの性能が著しく低下し（53.9→36.7）、異なる手法間の目的の競合を緩和する上で自己知識蒸留が不可欠であることが示されました。この結果は、論文のTable 2で詳細に示されています。 図3: 論文のTable 5にあるアブレーションスタディの結果 結論 BGE-M3の卓越した検索精度は、単一の手法が優れているからではなく、自己知識蒸留によって3つの異なる検索アプローチを相乗効果が生まれる形で巧みに統合している点にあります。この革新的なアプローチは、テキスト埋め込みモデルの新たな可能性を示唆しています。 引用 Shitao Xiao, Peitian Zhang, Zheng Liu, Xingyu He, Shihan Dou, Yuxuan Wang, Yajing Xu, Huangcan Li, Chao Zhang, and Jiangui Chen. 2024. BGE M3: Embedding Everything in a Single Model. arXiv preprint arXiv:2402.03216."
},

{
  "title": "LightRAGとHNSWの技術解説メモ",
  "url": "/articles/LightRAG/LightRAG.html",
  "date": "2025.08.13",
  "description": "LightRAGとHNSWの仕組みとアーキテクチャを解説したメモ。",
  "content": "このメモは、次世代RAGフレームワーク「LightRAG」と、その中核をなす高速ベクトル検索技術「HNSW」の仕組みについて、個人的にまとめたものです。 1. LightRAGの全体像とアーキテクチャ 1.1. 従来のRAGが抱える課題とLightRAGの解決策 従来のRAG（検索拡張生成）は、情報を単なるテキストの塊（チャンク）として扱うため、以下のような課題がありました。 情報の断片化: 複数の文書にまたがる複雑な質問に対し、文脈が途切れた断片的な回答しか生成できない。 関係性の欠如: 「A社がB社を買収した」と「B社の創設者は田中氏」という2つの情報から、「A社が田中氏を迎え入れた」という関係性を導き出すことが困難。 LightRAGは、この課題をナレッジグラフを用いて解決します。テキストからエンティティ（ノード）と関係性（エッジ）を抽出し、「意味の地図」として構造化することで、深い文脈理解と一貫性のある回答生成を可能にします。 1.2. LightRAGのアーキテクチャ：2つの頭脳の連携 LightRAGは、役割の異なる2つの主要なデータストア（頭脳）を連携させて動作します。以下の図は、その全体構造を示しています。 図1: LightRAGの構造 ナレッジグラフ・ストア (Knowledge Graph Store) 役割: 「意味を理解する脳」として機能します。ノード（例: A社）、エッジ（例: 買収した）、そしてそれらの構造的なつながりを保持します。データ間の論理的な関係性を管理する役割を担います。 構造: このグラフ自体は階層化されておらず、単一でフラットなネットワーク構造です。 ベクトルデータベース (Vector Database) 役割: 「超高速な反射神経」として機能します。ナレッジグラフ内の各要素（ノードやエッジの説明文など）をベクトル化して保存し、高速な類似検索を実現します。 内蔵エンジン: このデータベースの内部には、検索を高速化するための索引（インデックス）技術としてHNSWが組み込まれています。 1.3. LightRAGの動作フロー①：ナレッジグラフの構築 LightRAGは、以下のステップで非構造化テキストからナレッジグラフを構築します。 抽出 (Extraction): LLMを使い、テキストからエンティティ（人物、組織など）と、それらの間の関係性を抽出します。 プロファイリング (Profiling): 抽出した各ノードとエッジに対し、LLMが検索用のキーワードや要約文を付与します。この情報がベクトル化され、ベクトルデータベースに登録されます。 重複排除 (Deduplication): 新しく抽出されたエンティティが既存のものと同一かを判定します。同一であれば、新しいノードは作らず、既存のノードに情報を統合します。この判定には、エンティティの正規化された名前などから生成されるハッシュ値がIDとして用いられると考えられます。 1.4. LightRAGの動作フロー②：デュアルレベル検索 ユーザーからの質問に答える際、LightRAGは独自のデュアルレベル検索パラダイムを用います。 低レベル検索 (Low-Level Retrieval): 「A社のCEOは誰？」のような、具体的なエンティティに関する質問に対応します。質問から「A社」「CEO」といったキーワードを抽出し、特定のノードをターゲットに検索を行います。 高レベル検索 (High-Level Retrieval): 「近年のAI業界の動向は？」のような、広範で抽象的なテーマに関する質問に対応します。「AI業界」「動向」といったキーワードから、複数のノードやエッジ（関係性）を横断的に検索し、全体的な文脈を捉えます。 この2つの検索は、ベクトルデータベースのメタデータ（例: type='node', type='edge'）を利用して効率的に（多くの場合、非同期で）実行され、得られた結果をナレッジグラフ上で統合し、最終的な回答を生成します。 2. 中核技術HNSW：高速検索の心臓部 HNSW (Hierarchical Navigable Small World) は、膨大なベクトルデータの中から、目的のベクトルに最も近いものを高速に見つけ出すためのアルゴリズムです。 2.1. 階層化の仕組み：ランダム性こそが鍵 HNSWの最大の特徴は、その巧妙な階層化の仕組みです。以下の図は、その階層構造を模式的に示しています。 図2: HNSWの階層構造 階層決定は完全にランダム: 新しいデータ点が追加されるたびに、その点がどの階層まで所属するかは、そのデータの内容や重要度とは一切関係なく、サイコロを振るようにランダムな確率で決まります。階層が上がるほど、その階層に所属する確率は指数関数的に低くなります。 階層構造の役割: このランダム性により、自然と効率的な階層構造が生まれます。 上位階層: メンバーが非常に少ないため、データ空間全体を横断する「高速道路」のような長距離リンクが形成されます。 下位階層 (0階): 全てのメンバーが所属するため、「一般道」のような短距離リンクが密集したネットワークが形成されます。 2.2. 探索の仕組みと新規データ追加 HNSWは、この階層構造を活かして、以下の手順で探索を行います。 最上位から開始: 探索は、最もまばらな最上位レイヤーの「エントリーポイント」から始まります。 貪欲な探索 (Greedy Traversal): 現在いるノードから、リンクで繋がっている隣人の中で、最もクエリ（探したいデータ）に近い隣人へと移動します。この計算には、テキストベクトルの場合、コサイン類似度が一般的に用いられます。 下の階層へ: そのレイヤーで「これ以上近づけない」という局所的な最適解に達したら、その地点を新しい開始点として、一つ下の、より密なレイヤーに降りて探索を続けます。 繰り返し: このプロセスを最下層（0階）まで繰り返すことで、最終的な検索結果を得ます。 また、HNSWは増分更新が可能で、新しいデータが追加された際も、この探索アルゴリズムを使って自身の最適な場所を見つけ出し、ネットワークにスムーズに統合されます。 3. 個人的に気になった疑問と回答 ここでは、私がHNSWに関して抱いた疑問とその回答をまとめます。 Q: HNSWの上位レイヤーが「コアな単語」や「マイナーな単語」だけで構成された場合、探索はうまくいきますか？ A: はい、問題ありません。階層への所属はランダムであり、単語の重要度には依存しません。上位レイヤーの役割は、データ空間全体にバランス良く「ショートカット網（高速道路）」を構築することです。そのため、構成要素が何であれ、ナビゲーション機能は頑健に働きます。 Q: 探索中に極所解に陥ることはありますか？ A: 極めて陥りにくい設計になっています。その理由は、①階層決定のランダム性、②トップダウンの探索戦略、そして次に説明する③賢い隣人選びのヒューリスティックという3つのメカニズムが連携して機能するためです。 Q: 「多様な方向にある隣人を選ぶヒューリスティック」とは？ A: これは、リンクの「数」ではなく「質」と「配置」を工夫するルールです。単純に最も近い隣人とだけリンクを結ぶと、すべてのリンクが同じデータクラスター内に集中し、探索が孤立する危険があります。このヒューリスティックは、たとえ少し距離が遠くても、自分から見て多様な方向にある隣人とバランス良くリンクを結ぶことを促します。これにより、クラスター間に意図的に「橋」が架けられ、探索が袋小路に入るのを防ぎます。 Q: 各ノードのIDはハッシュ値ですか？ A: 論文に直接の記載はありません。しかし、LightRAGの重要機能である「重複排除（Deduplication）」を実現するためには、IDが「決定的（同じエンティティからは必ず同じIDが生成される）」である必要があります。この要件を最も満たすのがハッシュ値です。エンティティの正規化された名前などからハッシュ値を計算し、それを一意なIDとして使用していると考えるのが最も合理的です。 4. 参考文献 LightRAG: Guo, Z., Xia, L., Yu, Y., Ao, T., & Huang, C. (2024). LightRAG: Simple and Fast Retrieval-Augmented Generation. arXiv preprint arXiv:2410.05779. HNSW: Malkov, Y. A., & Yashunin, D. A. (2018). Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs. IEEE transactions on pattern analysis and machine intelligence, 42(4), 824-836."
},

{
  "title": "技術メモ：Transformerの位置符号（Positional Encoding）について",
  "url": "/articles/transformer-positional/transformer-positional.html",
  "date": "2025.08.13",
  "description": "Transformerの位置符号の重要性と計算方法を解説したメモ。",
  "content": "はじめに：本記事について Transformerモデルの内部で使用される位置符号（Positional Encoding）について、私が理解した内容をまとめます。特に、位置符号の計算方法やその背後にあるアイデアを深く掘り下げて解説します。 それぞれインプットに対して、位置符号をどのように計算するかを具体的に見ていきます。 図1: Transformerの構造 input encoding, output encodingの後の処理であるpositional encodingの部分の処理についてです。 なぜ位置符号が必要なのか？ Transformerモデルの心臓部であるSelf-Attentionは、文中の単語同士の関連度を計算する非常に強力な仕組みです。しかし、その計算過程で単語の元の順序が失われてしまうという大きな弱点があります。例えば、「犬が猫を追いかけた」と「猫が犬を追いかけた」では意味が全く異なりますが、Self-Attentionにとっては、単に「犬、猫、追いかけた、が、を」という単語の集合に見えてしまい、区別がつきません。 この問題を解決するのが位置符号（Positional Encoding）です。その目的は、入力される各単語のベクトルに対し、文章中におけるその単語の「住所」や「位置」に関する情報を、数学的に埋め込むことです。これにより、Transformerは単語の意味だけでなく、その順序も理解できるようになります。 第1章：位置符号の計算方法 1.1 計算式 位置符号は、文章中の単語の位置を $pos$ （0から始まる番号）、モデルが扱うベクトルの次元数を $d_{model}$ としたとき、ベクトルの各次元 $i$ に対して以下の式で計算されます。 偶数次元 ($2i$) の場合: $PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$ 奇数次元 ($2i+1$) の場合: $PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$ この計算によって、各位置 $pos$ ごとに、$d_{model}$ 次元のユニークなベクトル（位置符号ベクトル）が生成されます。最終的に、この位置符号ベクトルが、元の単語の意味を表すベクトル（単語埋め込み）に足し算されます。 1.2 具体例で学ぶ計算ステップ ($d_{model}=4$の場合) 文章「私は 猫が 好き」で、2番目の単語「猫」($pos=1$)の位置符号を計算してみましょう。 ベクトルの次元数 ($d_{model}$): 4次元 次元インデックス ($i$): $d_{model}/2 = 2$ なので、 $i=0$ と $i=1$ の2つのペアを考えます。 ① i=0のペア（0次元目, 1次元目）の計算 分母: $10000^{2 \\times 0 / 4} = 10000^0 = 1$ 0次元目(偶数): $PE_{(1, 0)} = \\sin(1/1) = \\sin(1) \\approx 0.841$ 1次元目(奇数): $PE_{(1, 1)} = \\cos(1/1) = \\cos(1) \\approx 0.540$ ② i=1のペア（2次元目, 3次元目）の計算 分母: $10000^{2 \\times 1 / 4} = 10000^{1/2} = 100$ 2次元目(偶数): $PE_{(1, 2)} = \\sin(1/100) \\approx 0.010$ 3次元目(奇数): $PE_{(1, 3)} = \\cos(1/100) \\approx 0.999$ ③ 結果の結合 「猫」($pos=1$)の位置符号ベクトルは、[0.841, 0.540, 0.010, 0.999]となります。同様に、他の位置の単語も全く異なるベクトルが生成され、位置を区別できます。 第2章：なぜこの仕組みなのか？ 核心概念の深掘り ここからは、対話の中で生まれた疑問とその答えを通して、この数式の裏にある巧みなアイデアを解説します。 2.1 なぜ次元ごとに波を変えるのか？ - インデックスiの役割 【疑問】 各次元のインデックス $i$ は何を意味しているのか？ $i$ は、位置符号ベクトルを構成する波のパターン（周波数）を変えるための「スイッチ」の役割を担っています。 計算式の分母 $10000^{2i/d_{model}}$ に注目すると、$i$ が小さいほど分母は小さく（1に近く）、$i$ が大きいほど分母は大きくなります。これは、sin/cosの中身に影響を与え、結果として波の揺れ方を変えます。 $i$ が小さい次元（ベクトルの前半）: 波の揺れがゆっくりになります（低周波）。これは、文章全体における大局的な位置関係を捉えるのに役立ちます。4次元の例では、0次元目と1次元目がこれにあたります。 $i$ が大きい次元（ベクトルの後半）: 波の揺れが速くなります（高周波）。これは、隣り合う単語同士の細かい、局所的な位置関係を捉えるのに役立ちます。4次元の例では、2次元目と3次元目がこれにあたります。 これは、様々な楽器が重なって一つの楽曲を作るオーケストラに似ています。低音担当の楽器（$i$ が小さい）と高音担当の楽器（$i$ が大きい）がそれぞれのメロディーを奏でることで、豊かで複雑なハーモニー（位置情報）が生まれるのです。 2.2 なぜsinとcosをペアにするのか？ - 「回転」という最強のアイデア 【疑問】 ペアにすることと「回転」の関係性がわからない。なぜペアにすると回転できるのか？ この疑問の答えこそ、位置符号の最もエレガントな部分です。 ペアで「円」を作る: もし1次元、例えば $\\sin(pos)$ だけで位置を表そうとすると、その値は数直線上を行ったり来たりするだけで、向きが一定しません。しかし、$\\cos(pos)$ と $\\sin(pos)$ をペアで使うと、これは2次元平面上の円周上の点の座標 (x, y) を表現することになります。$pos$ が増えることは、この円周上を点がクルクルと回転することを意味します。 回転で「相対位置」を測る: この「回転」という性質のおかげで、ある位置 $pos$ から「k個隣」の位置 $pos+k$ への移動は、どの $pos$ から出発しても、常に同じ角度だけの回転操作として表現できます。この回転操作は、数学的には回転行列 $R_k$ という「物差し」で表せます。 【理解の核心】 モデルが学習するのは、「位置2から位置3への行き方」や「位置8から位置9への行き方」といった無数の個別ケースではありません。モデルは、「\"1つ隣\"という関係性は、この $R_1$ という物差し（回転操作）を使えば測れる」という、たった一つの普遍的なルールを学習すれば良いのです。 これにより、モデルは文の長さや絶対位置に依存しない、非常に効率的で強力な「相対位置を認識する能力」を獲得します。 2.3 複数ペアはどう動くのか？ - 独立した世界での並列処理 【疑問】 4次元ベクトルの場合、2ペア分の回転ベクトルを「加える」ことで表現するのか？ これは非常に鋭い疑問です。答えは「加える」のではなく、「各ペアが独立して回転し、その結果を並べて結合（Concatenate）する」です。 4次元ベクトルは、2つの独立した「2次元の世界」だと考えます。 世界A ($i=0$ のペア): 大局的な位置を測る、周期の長い世界。 世界B ($i=1$ のペア): 局所的な位置を測る、周期の短い世界。 「k個隣」への移動を計算するとき、 世界Aの中で、専用の物差し $R_k(i=0)$ を使って回転が起こる。 それと並行して、全く独立に、世界Bの中でも専用の物差し $R_k(i=1)$ を使って回転が起こる。 最終的な4次元ベクトルは、世界Aの回転後の座標と、世界Bの回転後の座標を、単純に横に並べて作られます。 この「並列・独立処理」により、大局的な位置情報と局所的な位置情報の両方を失うことなく、4次元のベクトルとして保つことができるのです。 第3章：他の概念との関連と補足 3.1 フーリエ変換との関係 - 「思想の継承」 【疑問】 この仕組みはフーリエ変換に似ているか？ はい、これは非常に的確な洞察です。位置符号はフーリエ変換そのものではありませんが、その根底にある「どんな複雑な情報も、単純な波（sin/cos）の組み合わせで表現できる」という思想を色濃く受け継いでいます。 フーリエ変換が既存の信号を分析して周波数成分に分解するのに対し、位置符号は位置という情報を表現するために、様々な周波数の波を組み合わせてベクトルを人工的に生成する、という違いはありますが、その発想は共通しています。 3.2 「10000」という数字の謎 【疑問】 式に出てくる「10000」はどこから来たのか？マジックナンバーか？ はい、これは一種のマジックナンバーであり、Transformerの原論文の著者らが経験的に設定したハイパーパラメータです。この数字は、位置符号が表現できる波の周期（波長）の範囲を決定します。この値が大きすぎず小さすぎず、様々な長さの文章に対応できるバランスの良い値として選ばれ、現在では事実上の標準となっています。 おわりに 個人的な解釈のまとめが以下です。 「1番目のトークンと2番目のトークンのi=0のペアは同じ世界にいて、その中の物差しで自由にどのトークンも表せられる」 この一文は、位置符号の仕組みの神髄を完璧に捉えています。 共通の世界: 全ての単語は、次元ペア $i$ ごとに作られる「同じ座標平面」上にプロットされる。 共通の物差し: その世界の中では、相対距離を測るための「共通の回転行列（物差し）」が使い回される。 自由な表現: これにより、どの単語を基準にしても、他の全ての単語との相対的な位置関係を、普遍的なルールで自由に表現できる。 このエレガントな仕組みこそが、Transformerが複雑な文章構造を理解し、驚異的な性能を発揮するための根幹の一つとなっているのです。 参考文献 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In *Advances in neural information processing systems* (pp. 5998-6008)."
},

{
  "title": "Docker + GitLabで出る「invalid spec: :/tmp/ssh-agent.sock」エラーの原因と解決法",
  "url": "/articles/Gitlab-ssh-error/Gitlab-ssh-error.html",
  "date": "2025.05.29",
  "description": "Docker + GitLabで発生する ssh-agent.sock エラーの原因と解決法。",
  "content": "GitLab 上のプロジェクトを Docker コンテナの中からクローンやプッシュなどの操作を行う際、SSH 認証が必要になります。通常ローカルでは、ホスト側の SSH エージェント（ssh-agent）が秘密鍵を保持しており、そのソケットファイルへのパスが **SSH_AUTH_SOCK** です。 このソケットを Docker コンテナ内にも共有することで、Docker 内からもホストの SSH 認証情報を使って GitLab にアクセスできるようになります。 🔍 エラーの正体は？ GitLab リポジトリを Docker コンテナから操作しようとした際、以下のようなエラーが出ることがあります。 invalid spec: :/tmp/ssh-agent.sock: empty section between colons これは、Docker Compose で SSH エージェントのソケットをボリュームとしてマウントしようとした際に、環境変数 **SSH_AUTH_SOCK** が未設定だったために発生します。 📦 なぜ SSH_AUTH_SOCK が必要なのか？ Docker コンテナから GitLab（SSH接続）にアクセスするには、ホストマシンで動作している ssh-agent を通じて認証情報（秘密鍵）を使えるようにする必要があります。このとき、**SSH_AUTH_SOCK** がそのエージェントのソケットファイルの場所を指しています。 ⚠️ エラーの発生パターン Docker Compose などで以下のような指定をしている場合を考えます。 volumes: - ${SSH_AUTH_SOCK}:/tmp/ssh-agent.sock しかし、ホスト側で **SSH_AUTH_SOCK** が未設定（または空文字）だった場合、Docker Compose は次のように解釈します。 :/tmp/ssh-agent.sock これは「ホスト側パス」が空なので、Docker は不正なボリューム定義と判断してエラーを出力します。 ✅ 解決法：SSH_AUTH_SOCK を正しく設定する 以下のコマンドを実行して、**ssh-agent** を起動・設定します。 # 1. ssh-agent を起動\" eval \"$(ssh-agent -s)\" これは SSH 認証エージェントのデーモンを起動し、バックグラウンドで秘密鍵を保持してくれます。このコマンドにより **SSH_AUTH_SOCK** という変数に、ソケットファイルのパスが自動的にセットされます。 例: # 2. SSH_AUTH_SOCK を設定 export SSH_AUTH_SOCK=$(find /tmp/ssh-* -type s) # 3. 環境変数が正しく設定されたか確認 echo $SSH_AUTH_SOCK # => /tmp/ssh-abc123/agent.5835 その後、Docker Compose を再実行します。 これにより、**docker-compose.yml** の以下の指定が、次のように展開されます。 volumes: - ${SSH_AUTH_SOCK}:/tmp/ssh-agent.sock 展開後: /tmp/ssh-xyz123/agent.1234:/tmp/ssh-agent.sock つまり、「ホスト → コンテナ」間で SSH 認証情報の橋渡しができるようになります。結果として、Docker コンテナ内から GitLab への SSH アクセスが成功します。 💡 補足 この方法は Dockerfileを一切変更せず、環境変数だけで安全に対処できるため、CI/CD や本番環境でも安定して利用可能です。"
},

{
  "title": "Docker + Poetryで始めるPython開発環境構築",
  "url": "/articles/docler-poetry/docker-poetry.html",
  "date": "2025.05.28",
  "description": "Docker + PoetryでPython開発環境を構築する手順メモ。",
  "content": "Pythonプロジェクトの開発環境を整える際、依存関係の管理や環境の再現性は常に課題となります。 本記事では、この課題を解決するための強力な組み合わせ、**Docker**と**Poetry**を利用したPython開発環境の構築方法について解説します。 これにより、プロジェクトごとにクリーンで再現性の高い環境を簡単に構築し、開発を効率的に進めることができるようになります。 なぜDockerとPoetryを使うのか？ Dockerはアプリケーションをコンテナとして隔離し、環境依存の問題を解消します。これにより「私の環境では動くのに…」といった問題をなくし、チーム開発やデプロイをスムーズにします。 一方、PoetryはPythonプロジェクトの依存関係管理とパッケージングを強力にサポートします。仮想環境の作成から依存ライブラリのインストール、スクリプト実行まで一元的に管理できるため、pipやvenvといった既存のツールに比べて格段に使いやすさが向上します。 この二つを組み合わせることで、OSや既存のPython環境に影響を与えることなく、プロジェクトに最適な開発環境を構築できます。 環境構築のファイル構成 まずは、今回構築する環境のファイル構成を見てみましょう。 . ├── .devcontainer/ │ └── devcontainer.json ├── .vscode/ │ └── settings.json ├── app/ │ └── main.py ├── docker-compose.yml ├── Dockerfile ├── poetry.lock (初回起動時に自動生成) ├── pyproject.toml └── README.md この構成で、コンテナの定義、Pythonプロジェクトの設定、VS Codeの統合設定までをカバーしています。 Dockerfile: コンテナの設計図 `Dockerfile`は、Python 3.12をベースに、Poetry、必要なシステムライブラリ、そして開発に必要なPythonライブラリをインストールする手順を定義します。 ここでは、`matplotlib`、`numpy`、`torch`、`japanize-matplotlib`、そしてVS CodeでJupyter Notebookを利用するためのライブラリを事前にインストールしています。 FROM python:3.12-slim-bookworm # タイムゾーンの設定 (任意) ENV TZ=Asia/Tokyo RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Poetryのインストール RUN pip install poetry==1.8.2 # 作業ディレクトリの設定 WORKDIR /app # Poetryのパス設定 (PATHに追加) ENV PATH=\"/root/.local/bin:$PATH\" # 依存関係のキャッシュを効率化するために、pyproject.tomlとpoetry.lockを先にコピー COPY pyproject.toml poetry.lock* ./ # 必要なシステムライブラリをインストール # matplotlib, numpy, torch, japanize-matplotlib, Jupyter Notebook関連 RUN apt-get update && apt-get install -y \\ build-essential \\ git \\ libffi-dev \\ libssl-dev \\ zlib1g-dev \\ libjpeg-dev \\ && rm -rf /var/lib/apt/lists/* # Poetryの依存関係をインストール # ここでJupyter Notebook関連のライブラリもインストールされます RUN poetry install --no-root --no-interaction --no-ansi # アプリケーションコードのコピー COPY . . # コンテナ起動時のデフォルトコマンド (任意) # CMD [\"bash\"] コード1: Dockerfileの内容 pyproject.toml: プロジェクトの依存関係と設定 `pyproject.toml`はPoetryがプロジェクトの依存関係を管理するための中心的なファイルです。 今回は、前述のライブラリに加えて、Jupyter Notebook関連のライブラリも追加しています。 PyTorchについては、CPU版を想定したソースを指定しています。GPU版を使用する場合は、適宜変更してください。 [tool.poetry] name = \"my-python-project\" version = \"0.1.0\" description = \"\" authors = [\"Your Name &lt;you@example.com&gt;\"] readme = \"README.md\" [tool.poetry.dependencies] python = \"^3.12\" matplotlib = \"^3.8.4\" numpy = \"^1.26.4\" torch = {version = \"^2.2.2\", source = \"pytorch\"} japanize-matplotlib = \"^1.1.3\" ipykernel = \"^6.29.4\" jupyter = \"^1.0.0\" notebook = \"^7.1.3\" ipython = \"^8.23.0\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" [[tool.poetry.source]] name = \"pytorch\" url = \"https://download.pytorch.org/whl/cpu\" priority = \"explicit\" コード2: pyproject.tomlの内容 docker-compose.yml: サービス定義と起動設定 `docker-compose.yml`は、Dockerコンテナをまとめて管理するためのファイルです。 `Dockerfile`で定義したイメージを元に`app`サービスを構築し、ホストOSとコンテナ間でファイルを共有（ボリュームマウント）、Jupyter Notebook用にポートを公開しています。 version: '3.8' services: app: build: context: . dockerfile: Dockerfile volumes: - .:/app ports: - \"8888:8888\" # Jupyter Notebook用 working_dir: /app tty: true # コンテナ内でbashを継続させるために必要 コード3: docker-compose.ymlの内容 VS Code連携のための設定ファイル VS Codeの「Remote - Containers」拡張機能を使うと、コンテナ内部を直接開発環境として利用できます。 `.devcontainer/devcontainer.json`と`.vscode/settings.json`は、この連携をスムーズにするための設定です。 特に`devcontainer.json`では、コンテナ起動後に自動で依存関係をインストールする設定などを記述しています。 // .devcontainer/devcontainer.json { \"name\": \"Python 3.12 with Poetry\", \"dockerComposeFile\": \"../docker-compose.yml\", \"service\": \"app\", \"workspaceFolder\": \"/app\", \"customizations\": { \"vscode\": { \"extensions\": [ \"ms-python.python\", \"ms-toolsai.jupyter\" ], \"settings\": { \"python.defaultInterpreterPath\": \"/root/.local/share/pypoetry/venv/bin/python\", \"python.terminal.activateEnvironment\": true, \"jupyter.notebookFileRoot\": \"${workspaceFolder}\" } } }, \"postStartCommand\": \"poetry install --no-root\", \"remoteUser\": \"root\" } コード4: .devcontainer/devcontainer.jsonの内容 // .vscode/settings.json { \"python.languageServer\": \"Pylance\", \"python.analysis.autoImportCompletions\": true, \"python.terminal.activateEnvironment\": true, \"jupyter.notebookFileRoot\": \"${workspaceFolder}\" } コード5: .vscode/settings.jsonの内容 環境構築と使い方 ファイルの準備ができたら、いよいよ環境を構築し、使ってみましょう。 1. Docker Composeで環境を起動 プロジェクトのルートディレクトリで以下のコマンドを実行します。初回はイメージのビルドに時間がかかります。 docker compose up --build -d これにより、バックグラウンドでコンテナが起動します。 2. コンテナに入る 起動したコンテナ内で作業を行うには、以下のコマンドでシェルに入ります。 docker compose exec app bash Dockerコンテナのシェル内でできること `docker compose exec app bash` コマンドでコンテナのシェルに入った後、通常のLinux環境と同様に様々な操作が可能です。 **ファイル操作**: `ls`, `cd`, `pwd`, `cp`, `mv`, `rm`, `mkdir` など、基本的なファイル・ディレクトリ操作コマンドが利用できます。 **テキストエディタ**: `vi` や `nano` (Dockerfileでインストールしていれば) を使ってファイルを直接編集できます。 **プロセスの確認**: `ps aux` で現在実行中のプロセスを確認できます。 **ネットワークコマンド**: `ping`, `curl`, `wget` などで外部との通信を確認したり、リソースをダウンロードしたりできます。 **システム情報の確認**: `df -h` (ディスク使用量), `free -h` (メモリ使用量) などでコンテナのシステム状況を確認できます。 **Poetryコマンドの実行**: `poetry env list`: 作成された仮想環境のリストを表示します。 `poetry run python -c \"import sys; print(sys.version)\"`: コンテナ内のPythonバージョンを確認できます。 `poetry run &lt;コマンド&gt;`: Poetryが管理する仮想環境内で任意のコマンドを実行します。 **Pythonインタプリタの起動**: `poetry run python` でPythonの対話型シェルを起動し、コードを試すことができます。 **Jupyter Notebookの起動 (オプション)**: `docker-compose.yml`でポートを公開している場合、コンテナ内でJupyter Notebookを起動し、ホストのブラウザからアクセスできます。 poetry run jupyter lab --ip=0.0.0.0 --port=8888 --allow-root --no-browser 上記コマンド実行後、表示されるURL（トークン付き）をホストのブラウザで開くと、Jupyter Labにアクセスできます。 これらの操作を通じて、コンテナ内部の環境を探索し、デバッグや追加設定を行うことができます。 3. Poetryコマンドの利用 コンテナ内ではPoetryが利用可能です。 **新しいライブラリの追加**: `poetry add &lt;パッケージ名&gt;` **依存関係のインストール**: `poetry install` (pyproject.tomlに基づいて依存関係をインストール) **スクリプトの実行**: `poetry run python app/main.py` 4. VS Code Remote - Containersでの開発 最もおすすめの開発方法です。VS Codeに「Remote - Containers」拡張機能をインストールした後、以下の手順で開発を開始できます。 VS Codeを開き、コマンドパレット (`Ctrl+Shift+P` または `Cmd+Shift+P`) を開き、「Remote-Containers: Open Folder in Container...」を選択します。 このプロジェクトのルートディレクトリを選択します。 VS Codeが自動的にコンテナに接続し、その中で開発環境を開きます。ターミナルもコンテナ内につながり、Jupyter Notebookもシームレスに利用できます。 サンプルコード `app/main.py`に以下のサンプルコードを配置することで、`matplotlib`や`japanize-matplotlib`、`numpy`、`torch`が正しく動作するかを確認できます。 import matplotlib.pyplot as plt import japanize_matplotlib import numpy as np import torch def main(): print(\"Hello from Dockerized Poetry environment!\") print(f\"NumPy version: {np.__version__}\") print(f\"Torch version: {torch.__version__}\") # Matplotlibとjapanize-matplotlibのテスト x = np.linspace(0, 2 * np.pi, 100) y = np.sin(x) plt.figure(figsize=(8, 6)) plt.plot(x, y) plt.title(\"正弦波のグラフ（日本語対応）\") plt.xlabel(\"X軸\") plt.ylabel(\"Y軸\") plt.grid(True) # plt.show() # VS CodeのJupyterなどで実行する場合はコメントアウト if __name__ == \"__main__\": main() コード6: app/main.pyのサンプルコード `plt.show()`はGUI環境が必要なため、Jupyter Notebookやインタラクティブシェルで実行する場合はコメントアウトまたは適切に処理してください。 まとめと今後の展望 DockerとPoetryを組み合わせることで、Python開発の環境構築が格段に効率化され、プロジェクトの再現性と保守性が向上します。 特にVS CodeのRemote - Containers機能との連携は、開発体験を大きく向上させるでしょう。 このテンプレートを基に、皆さんのPythonプロジェクトがよりスムーズに進むことを願っています。 ご質問やさらなるカスタマイズのご要望があれば、お気軽にご連絡ください。"
},

{
  "title": "成人式の思い出と活動について",
  "url": "/articles/coming_of_age/coming_of_age.html",
  "date": "2021.01.10",
  "description": "成人式での活動や当時の思い出をまとめた記事。",
  "content": "2021年、新型コロナウイルスの影響で、私が住む越谷市では成人式が中止となってしまいました。 一生に一度の成人式が失われることに寂しさを感じ、何かできることはないかと考えていたとき、 元越谷市立西中学校のサッカー部のコーチや先生方と共に**「越谷ミレニアム&+1成人式@埼玉スタジアム」**を企画する機会を得ました。 活動概要 **Date**: 2021年2月28日 **Location**: 埼玉スタジアム2002 **Background**: 新型コロナウイルス感染症の影響による越谷市の成人式中止を受け、同級生と共に成人式を再企画。 **Role**: 実行委員長に声をかけられ、**書記・映像プロデューサー**として活動しました。 具体的には、会議での資料まとめ、ティザームービー作成、当日のオーロラビジョンでのスライドや映像の作成、 頂いた映像の編集、音響の調節などを担当しました。 **Funding**: クラウドファンディングにて**160万円**の資金を集めることに成功しました。 **Guest Messages**: 元KRUSH -65キロ級王者 HIROYAさん、元WBAスーパーフェザー級王者 山内 高志さん、 COLOR,DEEP,LUVANDSOUL KIKURIさん、DEEPリーダー TAKAさんからお祝いのメッセージを頂きました。 **Media Coverage**: 東部読売新聞に2回掲載されたほか、当日はNHKの取材が行われ、NHK首都圏ニュースにて放送されました。 活動を振り返って 困難な状況の中でも、仲間たちと協力して成人式を企画し、多くの人々の支援を得られたことは、 私にとってかけがえのない経験となりました。 特に、映像制作を通じてイベント全体の盛り上げに貢献できたこと、そして当日の参加者の笑顔を見たときの達成感は忘れられません。 この経験は、目標に向かってチームで協力することの重要性や、困難な状況を乗り越える力を私に与えてくれました。 以下に、当時の活動の様子を伝えるメディア掲載の画像や映像を掲載します。"
}

]
