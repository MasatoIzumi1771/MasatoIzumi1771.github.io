---
layout: layouts/article.njk
title: ViT（Vision Transformer）画像埋め込み解説
date: 2026-02-08
displayDate: 2026.02.08
permalink: articles/vit-embedding/index.html
tags:
  - articles
  - Transformer
  - 機械学習
  - ViT
  - 画像処理
description: Vision Transformer（ViT）における画像の埋め込み方法を解説。テキストとの対比を通じて、パッチ分割・線形投影・位置エンコーディングの仕組みを理解します。
update: ViT画像埋め込み記事を追加しました。
useMath: true
---

<p>
    本記事では、Vision Transformer（ViT）がどのように画像をベクトル化するかを解説します。テキストの埋め込みと対比しながら、画像特有の処理を理解していきます。
</p>

<h3>テキストと画像の対比</h3>

<pre><code>テキスト:
  "今日は晴れ" → トークン化 → ["今日", "は", "晴れ"] → 埋め込み層 → ベクトル列

画像:
  [画像データ] → パッチ分割 → [パッチ1, パッチ2, ...] → 線形投影 → ベクトル列</code></pre>

<p>
    <strong>画像では「パッチ」がトークンに相当します。</strong>
</p>

<h3>画像のパッチ分割</h3>

<h4>ステップ1: 画像をグリッド状に分割</h4>

<pre><code>元の画像 (224×224ピクセル)
┌────┬────┬────┬────┬────┬────┬────┐
│    │    │    │    │    │    │    │  ...
├────┼────┼────┼────┼────┼────┼────┤
│    │    │    │    │    │    │    │  ...
├────┼────┼────┼────┼────┼────┼────┤
...

16×16ピクセルのパッチに分割
→ 224 ÷ 16 = 14
→ 14 × 14 = 196パッチ</code></pre>

<h4>ステップ2: 各パッチをフラット化</h4>

<pre><code>1つのパッチ (16×16ピクセル、RGB 3チャンネル)
  → 16 × 16 × 3 = 768個の数値

パッチのピクセル値:
┌─────────────────┐
│ R G B R G B ... │  ← 1行目のピクセル
│ R G B R G B ... │  ← 2行目のピクセル
│ ...             │
└─────────────────┘
  ↓ フラット化
[r₀, g₀, b₀, r₁, g₁, b₁, ..., r₂₅₅, g₂₅₅, b₂₅₅]  ← 768次元ベクトル</code></pre>

<h3>線形投影（Patch Embedding）</h3>

<h4>テキストとの違い</h4>

<pre><code>テキスト:
  トークンID → 埋め込み層（テーブル参照）→ ベクトル
  ※ 離散的なID → ルックアップ

画像:
  パッチ（768次元）→ 線形投影（行列積）→ ベクトル（768次元など）
  ※ 連続的な数値 → 行列演算</code></pre>

<h4>線形投影の仕組み</h4>

$$
\text{出力} = \text{パッチベクトル} \times W + b
$$

<pre><code>パッチベクトル (768次元)     投影行列 W (768×768)      出力 (768次元)
[r₀, g₀, b₀, ..., b₂₅₅]  ×  [学習されるパラメータ]  =  [x₀, x₁, ..., x₇₆₇]

入力:  生のピクセル値（意味を持たない数値の羅列）
出力:  意味的な特徴を捉えたベクトル</code></pre>

<h4>なぜテーブル参照ではなく行列積か</h4>

<table>
    <thead>
        <tr>
            <th></th>
            <th>テキスト</th>
            <th>画像</th>
        </tr>
    </thead>
    <tbody>
        <tr><td>入力の性質</td><td>離散的（"猫" or "犬"、中間はない）</td><td>連続的（0〜255の任意の値）</td></tr>
        <tr><td>パターン数</td><td>有限（語彙5万語など）</td><td>無限（パッチのパターンは無限）</td></tr>
        <tr><td>埋め込み方法</td><td>テーブルで全パターンを持てる</td><td>関数（行列積）で変換する必要がある</td></tr>
    </tbody>
</table>

<h3>全体の流れ</h3>

<pre><code>画像 (224×224×3)
    ↓ パッチ分割 (16×16)
196個のパッチ (各16×16×3 = 768ピクセル)
    ↓ フラット化
196個のベクトル (各768次元)
    ↓ 線形投影 (学習パラメータ)
196個の埋め込みベクトル (各768次元)
    ↓ [CLS]トークンを先頭に追加
197個のベクトル
    ↓ 位置エンコーディング追加
197個の位置情報付きベクトル
    ↓ Transformer Encoder
197個の出力ベクトル
    ↓ [CLS]トークンの出力を使用
1つの画像埋め込み (768次元)</code></pre>

<h3>[CLS]トークンとは</h3>

<p>
    画像全体を代表するための特別なトークンです。
</p>

<pre><code>入力:
  [CLS] [パッチ1] [パッチ2] ... [パッチ196]
    ↓ Transformer (Self-Attention)
  [CLS'] [パッチ1'] [パッチ2'] ... [パッチ196']

[CLS]トークンはAttentionで全パッチの情報を集約
→ [CLS']が画像全体の表現になる</code></pre>

<h4>なぜ[CLS]を使うのか</h4>

<table>
    <thead>
        <tr>
            <th>方法</th>
            <th>説明</th>
            <th>欠点</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>全パッチの平均</td>
            <td>単純な平均プーリング</td>
            <td>重要な部分も重要でない部分も同じ重み</td>
        </tr>
        <tr>
            <td>[CLS]トークン</td>
            <td>Attentionで重要なパッチに注目</td>
            <td>学習で「どこを見るべきか」を獲得</td>
        </tr>
    </tbody>
</table>

<h3>位置エンコーディング（画像版）</h3>

<h4>なぜ必要か</h4>

<pre><code>パッチをフラット化すると位置情報が失われる:

元の画像:
  [顔]  [空]
  [体]  [木]

フラット化後:
  [顔] [空] [体] [木]  ← どれが隣同士かわからない</code></pre>

<h4>画像での位置エンコーディング</h4>

<table>
    <thead>
        <tr>
            <th>方法</th>
            <th>説明</th>
            <th>アプローチ</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>1次元の位置</td>
            <td>パッチ0, パッチ1, パッチ2, ...</td>
            <td>テキストと同じRoPE的アプローチ</td>
        </tr>
        <tr>
            <td>2次元の位置</td>
            <td>パッチ(0,0), パッチ(0,1), パッチ(1,0), ...</td>
            <td>MRoPE的アプローチ（高さと幅を別々に）</td>
        </tr>
    </tbody>
</table>

<h3>テキストと画像の埋め込み比較</h3>

<table>
    <thead>
        <tr>
            <th>項目</th>
            <th>テキスト</th>
            <th>画像 (ViT)</th>
        </tr>
    </thead>
    <tbody>
        <tr><td>入力単位</td><td>トークン（単語/サブワード）</td><td>パッチ（16×16ピクセル）</td></tr>
        <tr><td>入力形式</td><td>離散的（ID）</td><td>連続的（ピクセル値）</td></tr>
        <tr><td>埋め込み方法</td><td>テーブル参照</td><td>線形投影（行列積）</td></tr>
        <tr><td>位置情報</td><td>1次元（シーケンス位置）</td><td>1次元 or 2次元（行・列）</td></tr>
        <tr><td>代表ベクトル</td><td>[EOS]や平均</td><td>[CLS]や平均</td></tr>
    </tbody>
</table>

<h3>動的解像度（Qwen2-VL以降）</h3>

<p>
    従来のViTは全ての画像を固定サイズにリサイズしていましたが、最新のモデルでは動的解像度に対応しています。
</p>

<pre><code>従来のViT:
  全ての画像を224×224にリサイズ → 196パッチ固定

Qwen2-VL以降:
  画像サイズに応じてパッチ数が変動
  - 小さい画像: 少ないパッチ
  - 大きい画像: 多いパッチ（最大16,384パッチ）

  解像度は28の倍数に調整
  パッチサイズは14×14ピクセル</code></pre>

<h4>例</h4>

<pre><code>画像サイズ 448×448:
  → 448 ÷ 14 = 32
  → 32 × 32 = 1,024パッチ

画像サイズ 224×224:
  → 224 ÷ 14 = 16
  → 16 × 16 = 256パッチ</code></pre>

<h3>マルチモーダル（テキスト+画像）の統合</h3>

<pre><code>テキスト: "この画像に写っているのは"
画像:     [猫の画像]

処理:
  テキスト → トークン埋め込み → [T₁] [T₂] [T₃] [T₄] [T₅]
  画像     → パッチ埋め込み   → [I₁] [I₂] [I₃] ... [I₁₉₆]

統合（連結）:
  [T₁] [T₂] [T₃] [T₄] [T₅] [I₁] [I₂] [I₃] ... [I₁₉₆]
                            ↑
                        テキストと画像を同じシーケンスに

→ Transformerで一緒に処理
→ テキストと画像の関係を学習</code></pre>

<h3>まとめ</h3>

<pre><code>テキスト埋め込み:
  単語 → ID → テーブル参照 → ベクトル

画像埋め込み (ViT):
  画像 → パッチ分割 → 線形投影 → ベクトル

共通点:
  - 入力を「トークン列」として扱う
  - Transformerで処理できる形式に変換
  - 位置エンコーディングで位置情報を追加

違い:
  - テキストは離散（テーブル）、画像は連続（行列積）
  - 画像は2次元の位置情報を持つ</code></pre>

<h3>参考文献</h3>

<ul>
    <li>Dosovitskiy, A., et al. (2020). <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noopener">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a>. ICLR 2021.</li>
    <li>Wang, P., et al. (2024). <a href="https://arxiv.org/abs/2409.12191" target="_blank" rel="noopener">Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</a>. arXiv:2409.12191.</li>
</ul>
