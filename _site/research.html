<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>泉諒音 ポートフォリオ - 研究テーマ</title>
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Noto+Sans+JP:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <header class="header">
        <div class="header-inner">
            <h1 class="site-title"><a href="index.html">Masato Izumi Portfolio</a></h1>
            <nav class="nav">
                <ul class="nav-list">
                    <li class="nav-item"><a href="index.html">ホーム</a></li>
                    <li class="nav-item"><a href="research_achievements.html">研究実績</a></li>
                    <li class="nav-item"><a href="research.html">研究</a></li>
                    <li class="nav-item"><a href="skills.html">スキル</a></li>
                    <li class="nav-item"><a href="articles.html">研究記事</a></li>
                    <li class="nav-item"><a href="deep_research/index.html">Deep Research</a></li>
                    <li class="nav-item"><a href="other.html">その他</a></li>
                </ul>
            </nav>
            <button class="menu-toggle" aria-label="メニューを開く">
                <span class="menu-icon"></span>
            </button>
        </div>
    </header>

    <main class="main-content">
        <section id="my-research" class="section">
            <div class="container">
                <h2 class="section-title">私の研究テーマ</h2>
                <div class="research-content">
                    <p>
                        私の研究テーマは、**マルチモーダル処理を目指した自然言語処理モデルの潜在変数構造の解明**です。
                        近年、Transformerアーキテクチャを基盤としたBERTやGPT-2といった大規模な自然言語処理モデルが飛躍的な進化を遂げ、ChatGPTのような驚異的な性能を持つAIが登場しました。
                        しかし、これらのモデルは、その出力結果がどのようにして導かれるのか、内部のメカニズムが「ブラックボックス」として残されています。
                    </p>
                    <p>
                        私は、このブラックボックス化されたモデルが入力から生成する**潜在変数**に焦点を当て、その構造を解明することを目指しています。
                        潜在変数の構造を理解することで、モデルの再学習、出力結果の根拠の特定、さらにはモデルの**説明可能性（Explainable AI: XAI）**の向上に貢献できると考えています。
                        また、潜在変数を操作することで、異なるドメイン（例えば、テキストと画像）間での情報変換や統合を実現し、より高度な**マルチモーダル処理AI**の発展を目指しています。
                    </p>
                    <p>
                        具体的には、**BERT**や**GPT-2**などの代表的な自然言語処理モデルが生成する潜在変数について、深く研究を行っています。
                        文章は離散的な性質を持つため、単に数字の羅列である潜在変数だけを見てその意味を解釈することは困難です。
                        そこで、私は文章情報を連続的な画像に変換する**画像生成モデル**を開発し、その画像変化を分析することで、潜在変数空間の構造を視覚的に解明する独自の手法を採用しています。
                        このアプローチにより、言葉の持つ意味が潜在空間内でどのように表現され、相互に関連しているのかを直感的に捉えることが可能になります。
                    </p>
                    <p>
                        私の研究は、自然言語処理モデルの**透明性の向上**、**マルチモーダル処理の実現**、そして**説明可能なAI**の実現に貢献することを目指しており、より信頼性と応用性の高いAIシステムの開発に繋がるものと信じています。
                    </p>

                    <h3 class="subsection-title">関連研究分野</h3>
                    <ul class="related-fields">
                        <li>**深層学習**: CNN、RNN、AutoEncoder、Transformer、Attention</li>
                        <li>**自然言語処理**: 文章分類、著者分類、文章要約、タイトル生成、文章からの画像生成</li>
                        <li>**自然言語処理モデル**: BERT、Sentence-BERT、GPT-2、T5、Word2Vec、Doc2Vec</li>
                        <li>**画像処理**: 画像分類、画像モデル分類の精度向上、画像生成</li>
                        <li>**画像処理モデル**: ResNet、Vision Transformer</li>
                    </ul>
                </div>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 Masato Izumi Portfolio</p>
        </div>
    </footer>

    <script src="js/main.js"></script>
</body>
</html>